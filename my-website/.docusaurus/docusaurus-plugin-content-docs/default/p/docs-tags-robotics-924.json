{"tag":{"label":"robotics","permalink":"/docs/tags/robotics","allTagsPath":"/docs/tags","count":6,"items":[{"id":"module-4/intro","title":"Introduction to Vision-Language-Action (VLA)","description":"Introduction to Vision-Language-Action systems for humanoid robotics, covering voice-to-action pipelines, cognitive planning with LLMs, and autonomous humanoid systems.","permalink":"/docs/module-4/intro"},{"id":"module-3/chapter-2-isaac-ros-integration","title":"Isaac ROS Integration","description":"Learn about NVIDIA Isaac ROS integration focusing on accelerated perception pipelines and VSLAM for humanoid robotics applications.","permalink":"/docs/module-3/chapter-2-isaac-ros-integration"},{"id":"module-3/chapter-1-isaac-sim-essentials","title":"Isaac Sim Essentials","description":"Learn about NVIDIA Isaac Sim fundamentals including photorealistic rendering, physics simulation, and synthetic data generation for humanoid robotics.","permalink":"/docs/module-3/chapter-1-isaac-sim-essentials"},{"id":"module-3/chapter-3-nav2-humanoid-navigation","title":"Nav2 for Humanoid Navigation","description":"Learn about Navigation2 configuration for humanoid robots, focusing on path planning and navigation workflows adapted for bipedal locomotion.","permalink":"/docs/module-3/chapter-3-nav2-humanoid-navigation"},{"id":"module-4/quickstart","title":"VLA Module Quickstart","description":"Quickstart guide for the Vision-Language-Action module, covering voice-to-action pipelines, cognitive planning with LLMs, and autonomous humanoid systems.","permalink":"/docs/module-4/quickstart"},{"id":"module-4/chapter-1-voice-to-action","title":"Voice-to-Action Pipeline","description":"Learn how to implement voice-controlled robot systems using Whisper API for speech recognition and mapping natural language commands to ROS 2 actions.","permalink":"/docs/module-4/chapter-1-voice-to-action"}],"unlisted":false}}