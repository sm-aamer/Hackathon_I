"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[6130],{3365(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-2/chapter-3-sensor-simulation","title":"Chapter 3 - Sensor Simulation","description":"Introduction to Sensor Simulation in Robotics","source":"@site/docs/module-2/chapter-3-sensor-simulation.md","sourceDirName":"module-2","slug":"/module-2/chapter-3-sensor-simulation","permalink":"/docs/module-2/chapter-3-sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-2/chapter-3-sensor-simulation.md","tags":[],"version":"current","frontMatter":{"id":"chapter-3-sensor-simulation","sidebar_label":"Sensor Simulation","title":"Chapter 3 - Sensor Simulation"},"sidebar":"tutorialSidebar","previous":{"title":"Unity for Human-Robot Interaction","permalink":"/docs/module-2/chapter-2-unity-interaction"},"next":{"title":"Module 3 Quick Start","permalink":"/docs/module-3/quickstart"}}');var a=i(4848),o=i(8453);const r={id:"chapter-3-sensor-simulation",sidebar_label:"Sensor Simulation",title:"Chapter 3 - Sensor Simulation"},s="Chapter 3: Sensor Simulation",l={},d=[{value:"Introduction to Sensor Simulation in Robotics",id:"introduction-to-sensor-simulation-in-robotics",level:2},{value:"LiDAR Simulation",id:"lidar-simulation",level:2},{value:"LiDAR Sensor Characteristics",id:"lidar-sensor-characteristics",level:3},{value:"Gazebo LiDAR Simulation",id:"gazebo-lidar-simulation",level:3},{value:"Ray Sensor Plugin",id:"ray-sensor-plugin",level:4},{value:"Multi-Ray LiDAR Simulation",id:"multi-ray-lidar-simulation",level:4},{value:"Noise Modeling for LiDAR",id:"noise-modeling-for-lidar",level:4},{value:"Unity LiDAR Simulation",id:"unity-lidar-simulation",level:3},{value:"LiDAR Simulation Script",id:"lidar-simulation-script",level:4},{value:"Performance Optimization for LiDAR Simulation",id:"performance-optimization-for-lidar-simulation",level:4},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:2},{value:"RGB-D Camera Models",id:"rgb-d-camera-models",level:3},{value:"Depth Camera Configuration in Gazebo",id:"depth-camera-configuration-in-gazebo",level:4},{value:"Unity Depth Camera Simulation",id:"unity-depth-camera-simulation",level:4},{value:"Noise Models and Calibration",id:"noise-models-and-calibration",level:3},{value:"Depth Camera Noise Models",id:"depth-camera-noise-models",level:4},{value:"IMU Simulation",id:"imu-simulation",level:2},{value:"IMU Characteristics in Simulation",id:"imu-characteristics-in-simulation",level:3},{value:"IMU Sensor Model in Gazebo",id:"imu-sensor-model-in-gazebo",level:4},{value:"Unity IMU Simulation",id:"unity-imu-simulation",level:4},{value:"Temperature and Drift Effects",id:"temperature-and-drift-effects",level:3},{value:"Sensor Fusion in Simulation",id:"sensor-fusion-in-simulation",level:2},{value:"Data Integration Techniques",id:"data-integration-techniques",level:3},{value:"Kalman Filter for Sensor Fusion",id:"kalman-filter-for-sensor-fusion",level:4},{value:"Particle Filter Implementation",id:"particle-filter-implementation",level:4},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Example 1: Configuring a Multi-Sensor Robot in Gazebo",id:"example-1-configuring-a-multi-sensor-robot-in-gazebo",level:3},{value:"Example 2: Sensor Data Processing Pipeline",id:"example-2-sensor-data-processing-pipeline",level:3},{value:"Sample Sensor Data Outputs and Validation",id:"sample-sensor-data-outputs-and-validation",level:2},{value:"LiDAR Point Cloud Format",id:"lidar-point-cloud-format",level:3},{value:"Depth Image Format",id:"depth-image-format",level:3},{value:"Exercises for Students",id:"exercises-for-students",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"chapter-3-sensor-simulation",children:"Chapter 3: Sensor Simulation"})}),"\n",(0,a.jsx)(e.h2,{id:"introduction-to-sensor-simulation-in-robotics",children:"Introduction to Sensor Simulation in Robotics"}),"\n",(0,a.jsx)(e.p,{children:"Sensor simulation is a critical aspect of robotics development that allows for testing perception algorithms without the need for physical hardware. This chapter covers the simulation of various sensor types including LiDAR, depth cameras, and IMUs within Gazebo and Unity environments."}),"\n",(0,a.jsx)(e.p,{children:"Sensor simulation offers several advantages:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Cost-effective testing of perception algorithms"}),"\n",(0,a.jsx)(e.li,{children:"Controlled experimental conditions"}),"\n",(0,a.jsx)(e.li,{children:"Reproducible scenarios"}),"\n",(0,a.jsx)(e.li,{children:"Safe development of navigation and control systems"}),"\n",(0,a.jsx)(e.li,{children:"Accelerated development cycles"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Light Detection and Ranging (LiDAR) sensors are crucial for mapping and navigation in robotics. Simulating LiDAR data accurately is essential for developing and testing SLAM algorithms."}),"\n",(0,a.jsx)(e.h3,{id:"lidar-sensor-characteristics",children:"LiDAR Sensor Characteristics"}),"\n",(0,a.jsx)(e.p,{children:"LiDAR sensors produce 3D point clouds by measuring distances to objects using laser pulses. Key characteristics include:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Range: Typical ranges from 1m to 100m+ depending on sensor"}),"\n",(0,a.jsx)(e.li,{children:"Angular resolution: Horizontal and vertical resolution (e.g., 0.1\xb0 to 0.5\xb0)"}),"\n",(0,a.jsx)(e.li,{children:"Field of view: Horizontal (e.g., 360\xb0) and vertical (e.g., -30\xb0 to +10\xb0)"}),"\n",(0,a.jsx)(e.li,{children:"Scan frequency: Typically 5-20 Hz"}),"\n",(0,a.jsx)(e.li,{children:"Accuracy: Millimeter-level precision for close objects"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"gazebo-lidar-simulation",children:"Gazebo LiDAR Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Gazebo provides plugins for simulating various LiDAR sensors with realistic noise models and characteristics."}),"\n",(0,a.jsx)(e.h4,{id:"ray-sensor-plugin",children:"Ray Sensor Plugin"}),"\n",(0,a.jsx)(e.p,{children:"The Gazebo Ray plugin simulates LiDAR sensors using ray tracing:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="lidar_sensor" type="ray">\n  <always_on>true</always_on>\n  <update_rate>10</update_rate>\n  <pose>0 0 0.5 0 0 0</pose>\n  <visualize>true</visualize>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>720</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>\n        <max_angle>3.14159</max_angle>\n      </horizontal>\n      <vertical>\n        <samples>1</samples>\n        <resolution>1</resolution>\n        <min_angle>0</min_angle>\n        <max_angle>0</max_angle>\n      </vertical>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>30.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h4,{id:"multi-ray-lidar-simulation",children:"Multi-Ray LiDAR Simulation"}),"\n",(0,a.jsx)(e.p,{children:"For 3D LiDAR sensors with vertical beams:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="3d_lidar_sensor" type="ray">\n  <always_on>true</always_on>\n  <update_rate>10</update_rate>\n  <pose>0 0 0.5 0 0 0</pose>\n  <visualize>true</visualize>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>1440</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>\n        <max_angle>3.14159</max_angle>\n      </horizontal>\n      <vertical>\n        <samples>64</samples>\n        <resolution>1</resolution>\n        <min_angle>-0.5236</min_angle>  \x3c!-- -30 degrees --\x3e\n        <max_angle>0.1745</max_angle>    \x3c!-- 10 degrees --\x3e\n      </vertical>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>120.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h4,{id:"noise-modeling-for-lidar",children:"Noise Modeling for LiDAR"}),"\n",(0,a.jsx)(e.p,{children:"Adding realistic noise to LiDAR measurements:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="lidar_with_noise" type="ray">\n  \x3c!-- Previous configuration --\x3e\n  <noise>\n    <type>gaussian</type>\n    <mean>0.0</mean>\n    <stddev>0.01</stddev>  \x3c!-- 1cm standard deviation --\x3e\n  </noise>\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"unity-lidar-simulation",children:"Unity LiDAR Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Unity offers different approaches for simulating LiDAR data, often using raycasting techniques."}),"\n",(0,a.jsx)(e.h4,{id:"lidar-simulation-script",children:"LiDAR Simulation Script"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections.Generic;\n\npublic class LiDARSimulator : MonoBehaviour\n{\n    [Header("LiDAR Configuration")]\n    public int horizontalSamples = 720;\n    public int verticalSamples = 1;\n    public float minAngle = -Mathf.PI;\n    public float maxAngle = Mathf.PI;\n    public float verticalMinAngle = 0;\n    public float verticalMaxAngle = 0;\n    public float maxRange = 30.0f;\n    public LayerMask detectionLayers = -1;\n\n    [Header("Output")]\n    public bool visualizeRays = true;\n    public float pointSize = 0.05f;\n\n    private List<Vector3> pointCloud;\n    private float horizontalStep;\n    private float verticalStep;\n\n    void Start()\n    {\n        pointCloud = new List<Vector3>();\n        horizontalStep = (maxAngle - minAngle) / horizontalSamples;\n        verticalStep = (verticalMaxAngle - verticalMinAngle) / verticalSamples;\n    }\n\n    void FixedUpdate()\n    {\n        pointCloud.Clear();\n\n        for (int v = 0; v < verticalSamples; v++)\n        {\n            float vAngle = verticalMinAngle + (v * verticalStep);\n\n            for (int h = 0; h < horizontalSamples; h++)\n            {\n                float hAngle = minAngle + (h * horizontalStep);\n\n                Vector3 direction = GetDirection(hAngle, vAngle);\n                RaycastHit hit;\n\n                if (Physics.Raycast(transform.position, direction, out hit, maxRange, detectionLayers))\n                {\n                    pointCloud.Add(hit.point);\n\n                    if (visualizeRays)\n                    {\n                        Debug.DrawRay(transform.position, direction * hit.distance, Color.red);\n                    }\n                }\n                else\n                {\n                    // Add point at maximum range if no hit\n                    Vector3 farPoint = transform.position + direction * maxRange;\n                    pointCloud.Add(farPoint);\n\n                    if (visualizeRays)\n                    {\n                        Debug.DrawRay(transform.position, direction * maxRange, Color.blue);\n                    }\n                }\n            }\n        }\n    }\n\n    Vector3 GetDirection(float hAngle, float vAngle)\n    {\n        // Convert spherical coordinates to Cartesian\n        float x = Mathf.Cos(vAngle) * Mathf.Sin(hAngle);\n        float y = Mathf.Sin(vAngle);\n        float z = Mathf.Cos(vAngle) * Mathf.Cos(hAngle);\n\n        return new Vector3(x, y, z).normalized;\n    }\n\n    void OnRenderObject()\n    {\n        GL.Begin(GL.POINTS);\n        GL.Color(Color.red);\n\n        foreach (Vector3 point in pointCloud)\n        {\n            GL.Vertex(point);\n        }\n\n        GL.End();\n    }\n\n    // Method to get point cloud data\n    public Vector3[] GetPointCloud()\n    {\n        return pointCloud.ToArray();\n    }\n}\n'})}),"\n",(0,a.jsx)(e.h4,{id:"performance-optimization-for-lidar-simulation",children:"Performance Optimization for LiDAR Simulation"}),"\n",(0,a.jsx)(e.p,{children:"For real-time performance:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Reduce the number of raycasts during gameplay"}),"\n",(0,a.jsx)(e.li,{children:"Use spatial partitioning for collision detection"}),"\n",(0,a.jsx)(e.li,{children:"Implement level-of-detail for distant objects"}),"\n",(0,a.jsx)(e.li,{children:"Cache raycast results when possible"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Depth cameras provide 3D spatial information that is vital for perception and navigation tasks in robotics."}),"\n",(0,a.jsx)(e.h3,{id:"rgb-d-camera-models",children:"RGB-D Camera Models"}),"\n",(0,a.jsx)(e.p,{children:"Simulating both color and depth information for robotic vision applications."}),"\n",(0,a.jsx)(e.h4,{id:"depth-camera-configuration-in-gazebo",children:"Depth Camera Configuration in Gazebo"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="depth_camera" type="depth">\n  <always_on>true</always_on>\n  <update_rate>30</update_rate>\n  <camera>\n    <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees --\x3e\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10</far>\n    </clip>\n  </camera>\n  <plugin name="camera_controller" filename="libgazebo_ros_openni_kinect.so">\n    <baseline>0.2</baseline>\n    <alwaysOn>true</alwaysOn>\n    <updateRate>30.0</updateRate>\n    <cameraName>depth_camera</cameraName>\n    <imageTopicName>/rgb/image_raw</imageTopicName>\n    <depthImageTopicName>/depth/image_raw</depthImageTopicName>\n    <pointCloudTopicName>/depth/points</pointCloudTopicName>\n    <cameraInfoTopicName>/rgb/camera_info</cameraInfoTopicName>\n    <depthImageCameraInfoTopicName>/depth/camera_info</depthImageCameraInfoTopicName>\n    <frameName>depth_camera_frame</frameName>\n    <pointCloudCutoff>0.5</pointCloudCutoff>\n    <pointCloudCutoffMax>3.0</pointCloudCutoffMax>\n    <CxPrime>0</CxPrime>\n    <Cx>320.5</Cx>\n    <Cy>240.5</Cy>\n    <focalLength>320</focalLength>\n    <hackBaseline>0.07</hackBaseline>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h4,{id:"unity-depth-camera-simulation",children:"Unity Depth Camera Simulation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class DepthCameraSimulator : MonoBehaviour\n{\n    [Header("Camera Configuration")]\n    public int width = 640;\n    public int height = 480;\n    public float nearClip = 0.1f;\n    public float farClip = 10.0f;\n    public float fieldOfView = 60.0f;\n\n    [Header("Output")]\n    public RenderTexture depthTexture;\n    public Shader depthShader;\n\n    private Camera cam;\n    private Texture2D depthReadback;\n\n    void Start()\n    {\n        cam = GetComponent<Camera>();\n        SetupDepthCamera();\n    }\n\n    void SetupDepthCamera()\n    {\n        cam.fieldOfView = fieldOfView;\n        cam.nearClipPlane = nearClip;\n        cam.farClipPlane = farClip;\n\n        // Create depth texture\n        depthTexture = new RenderTexture(width, height, 24);\n        depthTexture.format = RenderTextureFormat.Depth;\n        depthTexture.Create();\n\n        cam.targetTexture = depthTexture;\n    }\n\n    // Get depth data as array\n    public float[,] GetDepthData()\n    {\n        if (depthReadback == null)\n            depthReadback = new Texture2D(width, height, TextureFormat.RGB24, false);\n\n        RenderTexture.active = depthTexture;\n        depthReadback.ReadPixels(new Rect(0, 0, width, height), 0, 0);\n        depthReadback.Apply();\n\n        Color[] pixels = depthReadback.GetPixels();\n        float[,] depthData = new float[height, width];\n\n        for (int y = 0; y < height; y++)\n        {\n            for (int x = 0; x < width; x++)\n            {\n                int pixelIndex = y * width + x;\n                float depthValue = pixels[pixelIndex].grayscale;\n                // Convert normalized depth to actual distance\n                depthData[y, x] = nearClip + depthValue * (farClip - nearClip);\n            }\n        }\n\n        return depthData;\n    }\n\n    // Convert depth data to point cloud\n    public Vector3[] DepthToPointCloud(float[,] depthData)\n    {\n        List<Vector3> points = new List<Vector3>();\n        float fovRad = fieldOfView * Mathf.Deg2Rad;\n        float aspectRatio = (float)width / height;\n\n        for (int y = 0; y < height; y++)\n        {\n            for (int x = 0; x < width; x++)\n            {\n                float depth = depthData[y, x];\n\n                if (depth > nearClip && depth < farClip)\n                {\n                    // Calculate normalized device coordinates (-1 to 1)\n                    float ndcX = (x / (float)width) * 2 - 1;\n                    float ndcY = (1 - y / (float)height) * 2 - 1;\n\n                    // Calculate world coordinates\n                    float tanHalfFov = Mathf.Tan(fovRad / 2);\n                    float xWorld = ndcX * tanHalfFov * aspectRatio * depth;\n                    float yWorld = ndcY * tanHalfFov * depth;\n\n                    Vector3 worldPoint = transform.position + transform.right * xWorld + transform.up * yWorld + transform.forward * depth;\n                    points.Add(worldPoint);\n                }\n            }\n        }\n\n        return points.ToArray();\n    }\n}\n'})}),"\n",(0,a.jsx)(e.h3,{id:"noise-models-and-calibration",children:"Noise Models and Calibration"}),"\n",(0,a.jsx)(e.p,{children:"Understanding and implementing realistic noise models for depth sensors."}),"\n",(0,a.jsx)(e.h4,{id:"depth-camera-noise-models",children:"Depth Camera Noise Models"}),"\n",(0,a.jsx)(e.p,{children:"Depth cameras exhibit several types of noise:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Quantization noise: Discrete depth values"}),"\n",(0,a.jsx)(e.li,{children:"Gaussian noise: Random measurement errors"}),"\n",(0,a.jsx)(e.li,{children:"Multiplicative noise: Noise increases with distance"}),"\n",(0,a.jsx)(e.li,{children:"Systematic errors: Calibration offsets"}),"\n"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Adding noise to depth camera in Gazebo --\x3e\n<sensor name="noisy_depth_camera" type="depth">\n  \x3c!-- Previous configuration --\x3e\n  <noise>\n    <type>gaussian</type>\n    <mean>0.0</mean>\n    <stddev>0.01</stddev>  \x3c!-- 1cm at 1m distance --\x3e\n  </noise>\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Inertial Measurement Units (IMUs) provide information about acceleration and orientation, which are essential for robot localization and control."}),"\n",(0,a.jsx)(e.h3,{id:"imu-characteristics-in-simulation",children:"IMU Characteristics in Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Simulating the various characteristics and noise profiles of real IMUs."}),"\n",(0,a.jsx)(e.h4,{id:"imu-sensor-model-in-gazebo",children:"IMU Sensor Model in Gazebo"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="imu_sensor" type="imu">\n  <always_on>true</always_on>\n  <update_rate>100</update_rate>\n  <pose>0 0 0.5 0 0 0</pose>\n  <topic>__default_topic__</topic>\n  <visualize>false</visualize>\n  <imu>\n    <angular_velocity>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.001</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.0001</bias_stddev>\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.001</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.0001</bias_stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.001</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.0001</bias_stddev>\n        </noise>\n      </z>\n    </angular_velocity>\n    <linear_acceleration>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.1</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.01</bias_stddev>\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.1</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.01</bias_stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.1</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.01</bias_stddev>\n        </noise>\n      </z>\n    </linear_acceleration>\n  </imu>\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h4,{id:"unity-imu-simulation",children:"Unity IMU Simulation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class IMUSimulator : MonoBehaviour\n{\n    [Header("IMU Configuration")]\n    public float accelerometerNoise = 0.1f;\n    public float gyroscopeNoise = 0.001f;\n    public float magnetometerNoise = 0.1f;\n\n    [Header("Calibration")]\n    public Vector3 accelerometerBias = Vector3.zero;\n    public Vector3 gyroscopeBias = Vector3.zero;\n    public Vector3 magnetometerBias = Vector3.zero;\n\n    private Rigidbody rb;\n    private Vector3 lastPosition;\n    private Quaternion lastRotation;\n\n    void Start()\n    {\n        rb = GetComponent<Rigidbody>();\n        if (!rb)\n        {\n            rb = gameObject.AddComponent<Rigidbody>();\n            rb.isKinematic = true; // We\'ll control position manually\n        }\n\n        lastPosition = transform.position;\n        lastRotation = transform.rotation;\n    }\n\n    void FixedUpdate()\n    {\n        // Calculate linear acceleration (approximate)\n        Vector3 linearAcceleration = (transform.position - lastPosition - lastPosition + lastPosition) / (Time.fixedDeltaTime * Time.fixedDeltaTime);\n\n        // Add noise to acceleration\n        linearAcceleration += AddNoise(Vector3.one * accelerometerNoise);\n        linearAcceleration += accelerometerBias;\n\n        // Calculate angular velocity (approximate)\n        Quaternion deltaRotation = transform.rotation * Quaternion.Inverse(lastRotation);\n        Vector3 angularVelocity = new Vector3(\n            Mathf.Atan2(2 * (deltaRotation.w * deltaRotation.x + deltaRotation.y * deltaRotation.z),\n                       1 - 2 * (deltaRotation.x * deltaRotation.x + deltaRotation.y * deltaRotation.y)) / Time.fixedDeltaTime,\n            Mathf.Atan2(2 * (deltaRotation.w * deltaRotation.y - deltaRotation.z * deltaRotation.x),\n                       1 - 2 * (deltaRotation.y * deltaRotation.y + deltaRotation.z * deltaRotation.z)) / Time.fixedDeltaTime,\n            Mathf.Atan2(2 * (deltaRotation.w * deltaRotation.z + deltaRotation.x * deltaRotation.y),\n                       1 - 2 * (deltaRotation.z * deltaRotation.z + deltaRotation.x * deltaRotation.x)) / Time.fixedDeltaTime\n        );\n\n        // Add noise to angular velocity\n        angularVelocity += AddNoise(Vector3.one * gyroscopeNoise);\n        angularVelocity += gyroscopeBias;\n\n        // Simulate magnetometer (Earth\'s magnetic field in local frame)\n        Vector3 magneticField = transform.InverseTransformDirection(Vector3.forward * 0.25f); // Approximate field\n        magneticField += AddNoise(Vector3.one * magnetometerNoise);\n        magneticField += magnetometerBias;\n\n        // Store for next frame\n        lastPosition = transform.position;\n        lastRotation = transform.rotation;\n\n        // Publish simulated IMU data\n        PublishIMUData(linearAcceleration, angularVelocity, magneticField);\n    }\n\n    Vector3 AddNoise(Vector3 noiseLevels)\n    {\n        return new Vector3(\n            Random.Range(-noiseLevels.x, noiseLevels.x),\n            Random.Range(-noiseLevels.y, noiseLevels.y),\n            Random.Range(-noiseLevels.z, noiseLevels.z)\n        );\n    }\n\n    void PublishIMUData(Vector3 linearAcc, Vector3 angularVel, Vector3 magField)\n    {\n        // In a real implementation, this would publish to ROS/other systems\n        Debug.Log($"IMU Data - Acc: {linearAcc}, Gyro: {angularVel}, Mag: {magField}");\n    }\n}\n'})}),"\n",(0,a.jsx)(e.h3,{id:"temperature-and-drift-effects",children:"Temperature and Drift Effects"}),"\n",(0,a.jsx)(e.p,{children:"Real IMUs exhibit temperature-dependent biases and drift over time:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Temperature effects in Gazebo IMU --\x3e\n<sensor name="thermal_imu" type="imu">\n  \x3c!-- Previous configuration --\x3e\n  <imu>\n    \x3c!-- Include thermal effects in noise model --\x3e\n    <angular_velocity>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.001</stddev>\n          <bias_mean>0.0</bias_mean>\n          <bias_stddev>0.0001</bias_stddev>\n          \x3c!-- Add thermal drift --\x3e\n          <dynamic_bias_stddev>0.00001</dynamic_bias_stddev>\n          <dynamic_bias_correlation_time>3600</dynamic_bias_correlation_time>\n        </noise>\n      </x>\n    </angular_velocity>\n  </imu>\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"sensor-fusion-in-simulation",children:"Sensor Fusion in Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Combining data from multiple sensors to create a more accurate understanding of the robot's environment and state."}),"\n",(0,a.jsx)(e.h3,{id:"data-integration-techniques",children:"Data Integration Techniques"}),"\n",(0,a.jsx)(e.p,{children:"Methods for combining sensor data in simulation environments."}),"\n",(0,a.jsx)(e.h4,{id:"kalman-filter-for-sensor-fusion",children:"Kalman Filter for Sensor Fusion"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-csharp",children:"using UnityEngine;\n\npublic class SensorFusionFilter : MonoBehaviour\n{\n    // State: [x, y, z, vx, vy, vz, ax, ay, az]\n    private Matrix state; // 9x1\n    private Matrix covariance; // 9x9\n\n    private float processNoise = 0.1f;\n    private float measurementNoise = 0.1f;\n\n    void Start()\n    {\n        // Initialize state and covariance matrices\n        state = new Matrix(9, 1);\n        covariance = new Matrix(9, 9);\n\n        // Initialize with identity matrix scaled by uncertainty\n        for (int i = 0; i < 9; i++)\n        {\n            covariance[i, i] = 1.0f; // Initial uncertainty\n        }\n    }\n\n    public void UpdateWithIMU(Vector3 linearAcc, Vector3 angularVel)\n    {\n        // Prediction step using IMU data\n        Predict(linearAcc, angularVel);\n    }\n\n    public void UpdateWithLiDAR(Vector3 positionMeasurement)\n    {\n        // Correction step using LiDAR position\n        Correct(positionMeasurement, 0, 1, 2); // x, y, z indices\n    }\n\n    public void UpdateWithDepthCamera(Vector3 positionMeasurement)\n    {\n        // Correction step using depth camera\n        Correct(positionMeasurement, 0, 1, 2); // x, y, z indices\n    }\n\n    private void Predict(Vector3 linearAcc, Vector3 angularVel)\n    {\n        // State transition model (simplified)\n        // Update position based on velocity and acceleration\n        // Update velocity based on acceleration\n        // Update acceleration based on IMU measurement\n\n        // Jacobian of state transition function\n        Matrix F = GetStateTransitionMatrix();\n\n        // Process noise covariance\n        Matrix Q = GetProcessNoiseCovariance();\n\n        // Predict state: x_k = F * x_k-1\n        state = F * state;\n\n        // Predict covariance: P_k = F * P_k-1 * F^T + Q\n        covariance = F * covariance * F.Transpose() + Q;\n    }\n\n    private void Correct(Vector3 measurement, params int[] measurementIndices)\n    {\n        // Measurement matrix H (maps state to measurement space)\n        Matrix H = GetMeasurementMatrix(measurementIndices);\n\n        // Innovation covariance\n        Matrix S = H * covariance * H.Transpose() + GetMeasurementNoiseCovariance();\n\n        // Kalman gain\n        Matrix K = covariance * H.Transpose() * S.Inverse();\n\n        // Innovation (difference between measurement and prediction)\n        Vector3 predictedMeasurement = GetPredictedMeasurement();\n        Matrix innovation = new Matrix(3, 1);\n        innovation[0, 0] = measurement.x - predictedMeasurement.x;\n        innovation[1, 0] = measurement.y - predictedMeasurement.y;\n        innovation[2, 0] = measurement.z - predictedMeasurement.z;\n\n        // Update state: x_k = x_k + K * innovation\n        state = state + K * innovation;\n\n        // Update covariance: P_k = (I - K * H) * P_k-1\n        Matrix I = Matrix.Identity(9);\n        covariance = (I - K * H) * covariance;\n    }\n\n    private Matrix GetStateTransitionMatrix()\n    {\n        // Simplified state transition matrix\n        Matrix F = Matrix.Identity(9);\n\n        // Fill in the appropriate values based on the motion model\n        // This is a simplified example - a real implementation would be more complex\n\n        return F;\n    }\n\n    private Matrix GetProcessNoiseCovariance()\n    {\n        Matrix Q = Matrix.Identity(9);\n        for (int i = 0; i < 9; i++)\n        {\n            Q[i, i] = processNoise;\n        }\n        return Q;\n    }\n\n    private Matrix GetMeasurementNoiseCovariance()\n    {\n        Matrix R = Matrix.Identity(3);\n        for (int i = 0; i < 3; i++)\n        {\n            R[i, i] = measurementNoise;\n        }\n        return R;\n    }\n\n    private Vector3 GetPredictedMeasurement()\n    {\n        // Extract position from state vector\n        return new Vector3(state[0, 0], state[1, 0], state[2, 0]);\n    }\n\n    private Matrix GetMeasurementMatrix(int[] indices)\n    {\n        Matrix H = new Matrix(indices.Length, 9);\n        for (int i = 0; i < indices.Length; i++)\n        {\n            H[i, indices[i]] = 1.0f;\n        }\n        return H;\n    }\n}\n"})}),"\n",(0,a.jsx)(e.h4,{id:"particle-filter-implementation",children:"Particle Filter Implementation"}),"\n",(0,a.jsx)(e.p,{children:"For non-linear/non-Gaussian systems:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-csharp",children:"using UnityEngine;\nusing System.Collections.Generic;\n\npublic class ParticleFilter : MonoBehaviour\n{\n    [System.Serializable]\n    public class Particle\n    {\n        public Vector3 position;\n        public Vector3 velocity;\n        public float weight;\n\n        public Particle(Vector3 pos, Vector3 vel, float w)\n        {\n            position = pos;\n            velocity = vel;\n            weight = w;\n        }\n    }\n\n    public List<Particle> particles = new List<Particle>();\n    public int particleCount = 100;\n\n    void Start()\n    {\n        InitializeParticles();\n    }\n\n    void InitializeParticles()\n    {\n        particles.Clear();\n        float weight = 1.0f / particleCount;\n\n        for (int i = 0; i < particleCount; i++)\n        {\n            Vector3 pos = transform.position + Random.insideUnitSphere * 0.5f; // Small uncertainty\n            Vector3 vel = Random.insideUnitSphere * 0.1f; // Small initial velocity\n            particles.Add(new Particle(pos, vel, weight));\n        }\n    }\n\n    public void Predict(Vector3 controlInput, float deltaTime)\n    {\n        foreach (var particle in particles)\n        {\n            // Apply motion model with noise\n            Vector3 noise = Random.insideUnitSphere * 0.01f; // Process noise\n            particle.position += particle.velocity * deltaTime + noise;\n            particle.velocity += controlInput * deltaTime;\n        }\n    }\n\n    public void UpdateWeights(Vector3 measurement, float measurementNoise)\n    {\n        float totalWeight = 0f;\n\n        foreach (var particle in particles)\n        {\n            // Calculate likelihood of measurement given particle state\n            float dist = Vector3.Distance(particle.position, measurement);\n            float likelihood = Mathf.Exp(-(dist * dist) / (2 * measurementNoise * measurementNoise));\n            particle.weight *= likelihood;\n            totalWeight += particle.weight;\n        }\n\n        // Normalize weights\n        if (totalWeight > 0)\n        {\n            foreach (var particle in particles)\n            {\n                particle.weight /= totalWeight;\n            }\n        }\n    }\n\n    public void Resample()\n    {\n        List<Particle> newParticles = new List<Particle>();\n        float weightSum = 0f;\n\n        // Calculate cumulative weights\n        List<float> cumulativeWeights = new List<float>();\n        foreach (var particle in particles)\n        {\n            weightSum += particle.weight;\n            cumulativeWeights.Add(weightSum);\n        }\n\n        // Resample particles\n        for (int i = 0; i < particleCount; i++)\n        {\n            float randomValue = Random.value * weightSum;\n            int index = 0;\n\n            // Find particle corresponding to random value\n            for (int j = 0; j < cumulativeWeights.Count; j++)\n            {\n                if (randomValue <= cumulativeWeights[j])\n                {\n                    index = j;\n                    break;\n                }\n            }\n\n            // Add particle with uniform weight\n            Particle selectedParticle = particles[index];\n            Vector3 noise = Random.insideUnitSphere * 0.001f; // Small resampling noise\n            newParticles.Add(new Particle(\n                selectedParticle.position + noise,\n                selectedParticle.velocity,\n                1.0f / particleCount\n            ));\n        }\n\n        particles = newParticles;\n    }\n\n    public Vector3 GetEstimatedState()\n    {\n        Vector3 weightedSum = Vector3.zero;\n        float totalWeight = 0f;\n\n        foreach (var particle in particles)\n        {\n            weightedSum += particle.position * particle.weight;\n            totalWeight += particle.weight;\n        }\n\n        if (totalWeight > 0)\n        {\n            return weightedSum / totalWeight;\n        }\n        else\n        {\n            return transform.position; // Fallback to current position\n        }\n    }\n}\n"})}),"\n",(0,a.jsx)(e.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,a.jsx)(e.h3,{id:"example-1-configuring-a-multi-sensor-robot-in-gazebo",children:"Example 1: Configuring a Multi-Sensor Robot in Gazebo"}),"\n",(0,a.jsx)(e.p,{children:"Complete example of a humanoid robot with multiple sensors:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<?xml version="1.0" ?>\n<sdf version="1.6">\n  <model name="humanoid_with_sensors">\n    \x3c!-- Base link --\x3e\n    <link name="base_link">\n      <pose>0 0 1.0 0 0 0</pose>\n      <inertial>\n        <mass>10.0</mass>\n        <inertia>\n          <ixx>0.5</ixx>\n          <iyy>0.5</iyy>\n          <izz>0.5</izz>\n        </inertia>\n      </inertial>\n\n      \x3c!-- Visual and collision --\x3e\n      <visual name="visual">\n        <geometry>\n          <box>\n            <size>0.3 0.3 0.8</size>\n          </box>\n        </geometry>\n        <material>\n          <ambient>0.8 0.8 0.8 1</ambient>\n          <diffuse>0.8 0.8 0.8 1</diffuse>\n        </material>\n      </visual>\n\n      <collision name="collision">\n        <geometry>\n          <box>\n            <size>0.3 0.3 0.8</size>\n          </box>\n        </geometry>\n      </collision>\n\n      \x3c!-- IMU Sensor --\x3e\n      <sensor name="imu_sensor" type="imu">\n        <always_on>true</always_on>\n        <update_rate>100</update_rate>\n        <pose>0 0 0.2 0 0 0</pose>\n        <topic>imu/data</topic>\n        <visualize>false</visualize>\n      </sensor>\n\n      \x3c!-- Depth Camera --\x3e\n      <sensor name="depth_camera" type="depth">\n        <always_on>true</always_on>\n        <update_rate>30</update_rate>\n        <pose>0.1 0 0.3 0 0 0</pose>\n        <topic>camera/depth</topic>\n        <camera>\n          <horizontal_fov>1.047</horizontal_fov>\n          <image>\n            <width>640</width>\n            <height>480</height>\n            <format>R8G8B8</format>\n          </image>\n          <clip>\n            <near>0.1</near>\n            <far>10</far>\n          </clip>\n        </camera>\n      </sensor>\n\n      \x3c!-- 2D LiDAR --\x3e\n      <sensor name="lidar_2d" type="ray">\n        <always_on>true</always_on>\n        <update_rate>10</update_rate>\n        <pose>0.1 0 0.4 0 0 0</pose>\n        <topic>lidar/scan</topic>\n        <ray>\n          <scan>\n            <horizontal>\n              <samples>720</samples>\n              <resolution>1</resolution>\n              <min_angle>-3.14159</min_angle>\n              <max_angle>3.14159</max_angle>\n            </horizontal>\n          </scan>\n          <range>\n            <min>0.1</min>\n            <max>30.0</max>\n            <resolution>0.01</resolution>\n          </range>\n        </ray>\n      </sensor>\n    </link>\n\n    \x3c!-- Head link with additional sensors --\x3e\n    <link name="head_link">\n      <pose>0 0 1.5 0 0 0</pose>\n      <inertial>\n        <mass>2.0</mass>\n        <inertia>\n          <ixx>0.1</ixx>\n          <iyy>0.1</iyy>\n          <izz>0.1</izz>\n        </inertia>\n      </inertial>\n\n      <visual name="visual">\n        <geometry>\n          <sphere>\n            <radius>0.15</radius>\n          </sphere>\n        </geometry>\n        <material>\n          <ambient>0.8 0.8 0.8 1</ambient>\n          <diffuse>0.8 0.8 0.8 1</diffuse>\n        </material>\n      </visual>\n\n      <collision name="collision">\n        <geometry>\n          <sphere>\n            <radius>0.15</radius>\n          </sphere>\n        </geometry>\n      </collision>\n\n      \x3c!-- 3D LiDAR on head --\x3e\n      <sensor name="lidar_3d" type="ray">\n        <always_on>true</always_on>\n        <update_rate>10</update_rate>\n        <pose>0 0 0.1 0 0 0</pose>\n        <topic>lidar_3d/scan</topic>\n        <ray>\n          <scan>\n            <horizontal>\n              <samples>1440</samples>\n              <resolution>1</resolution>\n              <min_angle>-3.14159</min_angle>\n              <max_angle>3.14159</max_angle>\n            </horizontal>\n            <vertical>\n              <samples>64</samples>\n              <resolution>1</resolution>\n              <min_angle>-0.5236</min_angle>\n              <max_angle>0.1745</max_angle>\n            </vertical>\n          </scan>\n          <range>\n            <min>0.1</min>\n            <max>120.0</max>\n            <resolution>0.01</resolution>\n          </range>\n        </ray>\n      </sensor>\n    </link>\n\n    \x3c!-- Joint between base and head --\x3e\n    <joint name="neck_joint" type="revolute">\n      <parent>base_link</parent>\n      <child>head_link</child>\n      <axis>\n        <xyz>0 1 0</xyz>\n        <limit>\n          <lower>-0.5</lower>\n          <upper>0.5</upper>\n          <effort>100</effort>\n          <velocity>1</velocity>\n        </limit>\n      </axis>\n      <pose>0 0 0.4 0 0 0</pose>\n    </joint>\n  </model>\n</sdf>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"example-2-sensor-data-processing-pipeline",children:"Example 2: Sensor Data Processing Pipeline"}),"\n",(0,a.jsx)(e.p,{children:"Unity script for processing multiple sensor inputs:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections.Generic;\n\npublic class SensorDataProcessor : MonoBehaviour\n{\n    [Header("Sensor References")]\n    public LiDARSimulator lidar;\n    public DepthCameraSimulator depthCam;\n    public IMUSimulator imu;\n\n    [Header("Fusion Settings")]\n    public float positionTrustFactor = 0.7f; // Higher trust in position sensors\n    public float orientationTrustFactor = 0.9f; // Higher trust in IMU for orientation\n\n    private Vector3 estimatedPosition;\n    private Quaternion estimatedOrientation;\n    private ParticleFilter particleFilter;\n    private SensorFusionFilter kalmanFilter;\n\n    void Start()\n    {\n        particleFilter = GetComponent<ParticleFilter>() ?? gameObject.AddComponent<ParticleFilter>();\n        kalmanFilter = GetComponent<SensorFusionFilter>() ?? gameObject.AddComponent<SensorFusionFilter>();\n\n        estimatedPosition = transform.position;\n        estimatedOrientation = transform.rotation;\n    }\n\n    void Update()\n    {\n        ProcessSensorData();\n    }\n\n    void ProcessSensorData()\n    {\n        // Get sensor measurements\n        Vector3[] lidarPoints = lidar?.GetPointCloud() ?? new Vector3[0];\n        float[,] depthData = depthCam?.GetDepthData() ?? null;\n        // IMU data is processed internally by the IMUSimulator\n\n        // Process LiDAR data\n        if (lidarPoints.Length > 0)\n        {\n            Vector3 lidarPosition = EstimatePositionFromLiDAR(lidarPoints);\n            particleFilter.UpdateWeights(lidarPosition, 0.2f); // 20cm uncertainty\n        }\n\n        // Process depth camera data\n        if (depthData != null)\n        {\n            Vector3 depthPosition = EstimatePositionFromDepth(depthData);\n            particleFilter.UpdateWeights(depthPosition, 0.3f); // 30cm uncertainty\n        }\n\n        // Resample particles based on weights\n        particleFilter.Resample();\n\n        // Get fused estimate\n        estimatedPosition = particleFilter.GetEstimatedState();\n        estimatedOrientation = GetFusedOrientation(); // Combine with IMU data\n\n        // Update robot pose\n        transform.position = estimatedPosition;\n        transform.rotation = estimatedOrientation;\n    }\n\n    Vector3 EstimatePositionFromLiDAR(Vector3[] points)\n    {\n        // Simple centroid calculation for demonstration\n        if (points.Length == 0) return transform.position;\n\n        Vector3 sum = Vector3.zero;\n        foreach (Vector3 point in points)\n        {\n            sum += point;\n        }\n\n        return sum / points.Length;\n    }\n\n    Vector3 EstimatePositionFromDepth(float[,] depthData)\n    {\n        // Convert depth data to position estimate\n        // This is a simplified approach - real implementation would be more complex\n        int height = depthData.GetLength(0);\n        int width = depthData.GetLength(1);\n\n        List<Vector3> validPoints = new List<Vector3>();\n        float threshold = 5.0f; // Ignore points beyond 5m\n\n        for (int y = 0; y < height; y++)\n        {\n            for (int x = 0; x < width; x++)\n            {\n                if (depthData[y, x] < threshold)\n                {\n                    // Convert pixel coordinates to world coordinates\n                    // This requires camera intrinsic parameters\n                    float worldX = (x - width / 2.0f) * depthData[y, x] * 0.001f; // Rough conversion\n                    float worldY = (height / 2.0f - y) * depthData[y, x] * 0.001f;\n                    float worldZ = depthData[y, x];\n\n                    Vector3 worldPoint = new Vector3(worldX, worldY, worldZ);\n                    validPoints.Add(transform.TransformPoint(worldPoint));\n                }\n            }\n        }\n\n        if (validPoints.Count > 0)\n        {\n            Vector3 sum = Vector3.zero;\n            foreach (Vector3 point in validPoints)\n            {\n                sum += point;\n            }\n            return sum / validPoints.Count;\n        }\n\n        return transform.position;\n    }\n\n    Quaternion GetFusedOrientation()\n    {\n        // In a real implementation, this would combine IMU data with other sources\n        // For now, we\'ll just return the current rotation\n        return transform.rotation;\n    }\n\n    // Method to get fused sensor data\n    public SensorData GetFusedData()\n    {\n        return new SensorData\n        {\n            position = estimatedPosition,\n            orientation = estimatedOrientation,\n            lidarPoints = lidar?.GetPointCloud(),\n            timestamp = Time.time\n        };\n    }\n\n    [System.Serializable]\n    public struct SensorData\n    {\n        public Vector3 position;\n        public Quaternion orientation;\n        public Vector3[] lidarPoints;\n        public float timestamp;\n    }\n}\n'})}),"\n",(0,a.jsx)(e.h2,{id:"sample-sensor-data-outputs-and-validation",children:"Sample Sensor Data Outputs and Validation"}),"\n",(0,a.jsx)(e.h3,{id:"lidar-point-cloud-format",children:"LiDAR Point Cloud Format"}),"\n",(0,a.jsx)(e.p,{children:"LiDAR sensors typically output data in PointCloud2 format:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:'# Example PointCloud2 message structure\nheader:\n  seq: 1234\n  stamp:\n    secs: 1234\n    nsecs: 567890\n  frame_id: "lidar_frame"\nheight: 1\nwidth: 720\nfields:\n  - name: "x"\n    offset: 0\n    datatype: 7  # FLOAT32\n    count: 1\n  - name: "y"\n    offset: 4\n    datatype: 7  # FLOAT32\n    count: 1\n  - name: "z"\n    offset: 8\n    datatype: 7  # FLOAT32\n    count: 1\nis_bigendian: False\npoint_step: 16  # 4 bytes per float * 4 fields (x,y,z,intensity)\nrow_step: 11520  # 16 bytes * 720 points\ndata: [720*16 bytes of binary data...]\nis_dense: False\n'})}),"\n",(0,a.jsx)(e.h3,{id:"depth-image-format",children:"Depth Image Format"}),"\n",(0,a.jsx)(e.p,{children:"Depth cameras output images in various formats:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:'# Example depth image message\nheader:\n  seq: 1234\n  stamp:\n    secs: 1234\n    nsecs: 567890\n  frame_id: "camera_frame"\nheight: 480\nwidth: 640\nencoding: "32FC1"  # 32-bit float, 1 channel\nis_bigendian: False\nstep: 2560  # 640 pixels * 4 bytes per float\ndata: [640*480*4 bytes of binary depth data...]\n'})}),"\n",(0,a.jsx)(e.h2,{id:"exercises-for-students",children:"Exercises for Students"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Create a Gazebo world with a humanoid robot equipped with LiDAR, depth camera, and IMU sensors"}),"\n",(0,a.jsx)(e.li,{children:"Implement a simple SLAM algorithm using the simulated sensor data"}),"\n",(0,a.jsx)(e.li,{children:"Compare the performance of different sensor fusion techniques (Kalman filter vs. particle filter)"}),"\n",(0,a.jsx)(e.li,{children:"Add noise to the simulated sensors and analyze its effect on perception accuracy"}),"\n",(0,a.jsx)(e.li,{children:"Create a simulation scenario where the robot must navigate using only sensor data"}),"\n",(0,a.jsx)(e.li,{children:"Implement a sensor validation system that detects and filters out anomalous sensor readings"}),"\n",(0,a.jsx)(e.li,{children:"Design a sensor placement strategy that maximizes perception effectiveness for humanoid navigation"}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(c,{...n})}):c(n)}},8453(n,e,i){i.d(e,{R:()=>r,x:()=>s});var t=i(6540);const a={},o=t.createContext(a);function r(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:r(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);