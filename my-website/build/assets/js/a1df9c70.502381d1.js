"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[1118],{8453(e,n,r){r.d(n,{R:()=>t,x:()=>o});var i=r(6540);const s={},a=i.createContext(s);function t(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),i.createElement(a.Provider,{value:n},e.children)}},9064(e,n,r){r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-3/chapter-2-isaac-ros-integration","title":"Isaac ROS Integration","description":"Learn about NVIDIA Isaac ROS integration focusing on accelerated perception pipelines and VSLAM for humanoid robotics applications.","source":"@site/docs/module-3/chapter-2-isaac-ros-integration.md","sourceDirName":"module-3","slug":"/module-3/chapter-2-isaac-ros-integration","permalink":"/docs/module-3/chapter-2-isaac-ros-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-3/chapter-2-isaac-ros-integration.md","tags":[{"inline":true,"label":"isaac-ros","permalink":"/docs/tags/isaac-ros"},{"inline":true,"label":"ros-integration","permalink":"/docs/tags/ros-integration"},{"inline":true,"label":"perception-pipelines","permalink":"/docs/tags/perception-pipelines"},{"inline":true,"label":"vslam","permalink":"/docs/tags/vslam"},{"inline":true,"label":"gpu-acceleration","permalink":"/docs/tags/gpu-acceleration"},{"inline":true,"label":"robotics","permalink":"/docs/tags/robotics"}],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Isaac ROS Integration","sidebar_position":2,"tags":["isaac-ros","ros-integration","perception-pipelines","vslam","gpu-acceleration","robotics"],"description":"Learn about NVIDIA Isaac ROS integration focusing on accelerated perception pipelines and VSLAM for humanoid robotics applications."},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim Essentials","permalink":"/docs/module-3/chapter-1-isaac-sim-essentials"},"next":{"title":"Nav2 for Humanoid Navigation","permalink":"/docs/module-3/chapter-3-nav2-humanoid-navigation"}}');var s=r(4848),a=r(8453);const t={title:"Isaac ROS Integration",sidebar_position:2,tags:["isaac-ros","ros-integration","perception-pipelines","vslam","gpu-acceleration","robotics"],description:"Learn about NVIDIA Isaac ROS integration focusing on accelerated perception pipelines and VSLAM for humanoid robotics applications."},o="Isaac ROS Integration",l={},c=[{value:"Overview of Isaac ROS",id:"overview-of-isaac-ros",level:2},{value:"Cross-Module Connections",id:"cross-module-connections",level:2},{value:"Key Benefits of Isaac ROS",id:"key-benefits-of-isaac-ros",level:2},{value:"Performance Advantages",id:"performance-advantages",level:3},{value:"Hardware Acceleration Features",id:"hardware-acceleration-features",level:3},{value:"Isaac ROS Perception Pipelines",id:"isaac-ros-perception-pipelines",level:2},{value:"Accelerated Perception Nodes",id:"accelerated-perception-nodes",level:3},{value:"1. Isaac ROS Image Pipeline",id:"1-isaac-ros-image-pipeline",level:4},{value:"2. Isaac ROS Stereo Disparity",id:"2-isaac-ros-stereo-disparity",level:4},{value:"3. Isaac ROS Visual SLAM",id:"3-isaac-ros-visual-slam",level:4},{value:"Perception Pipeline Architecture",id:"perception-pipeline-architecture",level:3},{value:"VSLAM Acceleration on NVIDIA Hardware",id:"vslam-acceleration-on-nvidia-hardware",level:2},{value:"Visual SLAM Fundamentals",id:"visual-slam-fundamentals",level:3},{value:"Isaac ROS VSLAM Components",id:"isaac-ros-vslam-components",level:3},{value:"1. Feature Detection and Matching",id:"1-feature-detection-and-matching",level:4},{value:"2. Tracking and Mapping",id:"2-tracking-and-mapping",level:4},{value:"3. Loop Closure Detection",id:"3-loop-closure-detection",level:4},{value:"Performance Comparisons",id:"performance-comparisons",level:3},{value:"Isaac ROS Sensor Processing",id:"isaac-ros-sensor-processing",level:2},{value:"Camera Integration",id:"camera-integration",level:3},{value:"Monocular Cameras",id:"monocular-cameras",level:4},{value:"Stereo Cameras",id:"stereo-cameras",level:4},{value:"RGB-D Cameras",id:"rgb-d-cameras",level:4},{value:"LIDAR Integration",id:"lidar-integration",level:3},{value:"Point Cloud Processing",id:"point-cloud-processing",level:4},{value:"Obstacle Detection",id:"obstacle-detection",level:4},{value:"Practical Examples: Isaac ROS Perception Implementation",id:"practical-examples-isaac-ros-perception-implementation",level:2},{value:"Example 1: Accelerated Image Pipeline",id:"example-1-accelerated-image-pipeline",level:3},{value:"Example 2: Isaac ROS VSLAM Node",id:"example-2-isaac-ros-vslam-node",level:3},{value:"Example 3: Isaac ROS Deep Learning Pipeline",id:"example-3-isaac-ros-deep-learning-pipeline",level:3},{value:"Configuration Examples for Isaac ROS Pipelines",id:"configuration-examples-for-isaac-ros-pipelines",level:2},{value:"1. Isaac ROS Launch Configuration",id:"1-isaac-ros-launch-configuration",level:3},{value:"2. Isaac ROS Parameter Configuration",id:"2-isaac-ros-parameter-configuration",level:3},{value:"Troubleshooting Isaac ROS Integration",id:"troubleshooting-isaac-ros-integration",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"1. GPU Memory Issues",id:"1-gpu-memory-issues",level:4},{value:"2. CUDA Compatibility Issues",id:"2-cuda-compatibility-issues",level:4},{value:"3. Performance Bottlenecks",id:"3-performance-bottlenecks",level:4},{value:"4. Calibration Issues",id:"4-calibration-issues",level:4},{value:"Performance Optimization Tips",id:"performance-optimization-tips",level:3},{value:"1. Pipeline Optimization",id:"1-pipeline-optimization",level:4},{value:"2. Model Optimization",id:"2-model-optimization",level:4},{value:"3. Resource Management",id:"3-resource-management",level:4},{value:"Exercises for Isaac ROS Integration",id:"exercises-for-isaac-ros-integration",level:2},{value:"Exercise 1: Basic Isaac ROS Pipeline Setup",id:"exercise-1-basic-isaac-ros-pipeline-setup",level:3},{value:"Exercise 2: VSLAM Integration",id:"exercise-2-vslam-integration",level:3},{value:"Exercise 3: Accelerated Object Detection",id:"exercise-3-accelerated-object-detection",level:3},{value:"Exercise 4: Multi-Sensor Fusion",id:"exercise-4-multi-sensor-fusion",level:3},{value:"Exercise 5: Custom Perception Pipeline",id:"exercise-5-custom-perception-pipeline",level:3},{value:"Exercise 6: Performance Optimization",id:"exercise-6-performance-optimization",level:3},{value:"Exercise 7: Isaac ROS Deployment",id:"exercise-7-isaac-ros-deployment",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"isaac-ros-integration",children:"Isaac ROS Integration"})}),"\n",(0,s.jsx)(n.p,{children:"This chapter covers the integration of NVIDIA Isaac with ROS (Robot Operating System), focusing on accelerated perception pipelines and VSLAM (Visual Simultaneous Localization and Mapping) for humanoid robotics applications. Isaac ROS leverages NVIDIA's GPU acceleration to dramatically improve the performance of robotics perception tasks."}),"\n",(0,s.jsx)(n.h2,{id:"overview-of-isaac-ros",children:"Overview of Isaac ROS"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS is NVIDIA's collection of hardware-accelerated perception and navigation packages designed to seamlessly integrate with ROS 2. It provides:"}),"\n",(0,s.jsxs)(n.p,{children:["Isaac ROS directly extends the ROS 2 concepts introduced in ",(0,s.jsx)(n.a,{href:"/docs/module-1/chapter-1-ros2-fundamentals",children:"Module 1: The Robotic Nervous System"})," by adding GPU-accelerated processing capabilities. The perception pipelines in Isaac ROS can process the sensor data formats you learned about in ",(0,s.jsx)(n.a,{href:"/docs/module-2/chapter-3-sensor-simulation",children:"Module 2: The Digital Twin"}),", but with dramatically improved performance."]}),"\n",(0,s.jsx)(n.h2,{id:"cross-module-connections",children:"Cross-Module Connections"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"With Module 1 (ROS 2)"}),": Isaac ROS builds upon standard ROS 2 interfaces and message types, extending them with GPU-accelerated processing. The nodes, topics, and services you learned about in Module 1 work seamlessly with Isaac ROS."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"With Module 2 (Digital Twin)"}),": Isaac ROS can process sensor data from simulated environments created in Gazebo or Unity, providing accelerated perception that bridges the gap between simulation and real-world performance."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"GPU-accelerated perception"}),": Leveraging CUDA and TensorRT for real-time processing"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Hardware-accelerated VSLAM"}),": Utilizing NVIDIA GPUs for simultaneous localization and mapping"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Deep learning inference"}),": Accelerated neural network inference for AI-powered perception"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Sensor processing"}),": Optimized pipelines for cameras, LIDAR, and other sensors"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 compatibility"}),": Full integration with the ROS 2 ecosystem"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"key-benefits-of-isaac-ros",children:"Key Benefits of Isaac ROS"}),"\n",(0,s.jsx)(n.h3,{id:"performance-advantages",children:"Performance Advantages"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Up to 10x faster processing"}),": Compared to CPU-only implementations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time perception"}),": Enabling complex AI algorithms to run in real-time"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reduced latency"}),": Critical for responsive robot behavior"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Energy efficiency"}),": Optimized processing reduces power consumption"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"hardware-acceleration-features",children:"Hardware Acceleration Features"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CUDA optimization"}),": Direct integration with NVIDIA GPU cores"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"TensorRT acceleration"}),": Optimized deep learning inference"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Video Processing Units (VPUs)"}),": Dedicated hardware for video processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hardware video decoding"}),": Direct GPU decoding of camera streams"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"isaac-ros-perception-pipelines",children:"Isaac ROS Perception Pipelines"}),"\n",(0,s.jsx)(n.h3,{id:"accelerated-perception-nodes",children:"Accelerated Perception Nodes"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS provides a variety of perception nodes optimized for GPU acceleration:"}),"\n",(0,s.jsx)(n.h4,{id:"1-isaac-ros-image-pipeline",children:"1. Isaac ROS Image Pipeline"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hardware-accelerated image decoding"}),": Direct GPU decoding of compressed images"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Color space conversion"}),": GPU-accelerated color space transformations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Image rectification"}),": Real-time stereo rectification for depth estimation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resize and crop"}),": Hardware-accelerated image scaling operations"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"2-isaac-ros-stereo-disparity",children:"2. Isaac ROS Stereo Disparity"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time stereo matching"}),": Accelerated disparity computation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Subpixel refinement"}),": GPU-enhanced precision for depth maps"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Filtering and post-processing"}),": Hardware-accelerated noise reduction"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"3-isaac-ros-visual-slam",children:"3. Isaac ROS Visual SLAM"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature extraction"}),": GPU-accelerated feature detection and description"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pose estimation"}),": Real-time camera pose calculation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Map building"}),": Accelerated 3D map construction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Loop closure"}),": Hardware-accelerated map optimization"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"perception-pipeline-architecture",children:"Perception Pipeline Architecture"}),"\n",(0,s.jsx)(n.p,{children:"The Isaac ROS perception pipeline follows a modular architecture:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Camera Input \u2192 Isaac ROS Image Pipeline \u2192 Feature Extraction \u2192\r\nPose Estimation \u2192 Map Building \u2192 Localized Output\n"})}),"\n",(0,s.jsx)(n.p,{children:"Each stage benefits from hardware acceleration, resulting in significantly improved performance compared to traditional CPU-based approaches."}),"\n",(0,s.jsx)(n.h2,{id:"vslam-acceleration-on-nvidia-hardware",children:"VSLAM Acceleration on NVIDIA Hardware"}),"\n",(0,s.jsx)(n.h3,{id:"visual-slam-fundamentals",children:"Visual SLAM Fundamentals"}),"\n",(0,s.jsx)(n.p,{children:"Visual SLAM (Simultaneous Localization and Mapping) enables robots to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Build a map of their environment using visual sensors"}),"\n",(0,s.jsx)(n.li,{children:"Determine their position within that map simultaneously"}),"\n",(0,s.jsx)(n.li,{children:"Navigate autonomously in unknown environments"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"isaac-ros-vslam-components",children:"Isaac ROS VSLAM Components"}),"\n",(0,s.jsx)(n.h4,{id:"1-feature-detection-and-matching",children:"1. Feature Detection and Matching"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Accelerated feature detectors"}),": FAST, ORB, SIFT variants optimized for GPU"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Descriptor computation"}),": Hardware-accelerated descriptor generation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature matching"}),": GPU-based nearest neighbor searches"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"2-tracking-and-mapping",children:"2. Tracking and Mapping"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Camera pose estimation"}),": Real-time pose calculation using GPU"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Keyframe selection"}),": Accelerated keyframe evaluation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Bundle adjustment"}),": Hardware-optimized map refinement"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"3-loop-closure-detection",children:"3. Loop Closure Detection"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Place recognition"}),": GPU-accelerated place identification"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Map optimization"}),": Accelerated graph optimization for loop closure"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-comparisons",children:"Performance Comparisons"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Component"}),(0,s.jsx)(n.th,{children:"CPU Performance"}),(0,s.jsx)(n.th,{children:"Isaac ROS GPU Performance"}),(0,s.jsx)(n.th,{children:"Improvement"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Feature Detection"}),(0,s.jsx)(n.td,{children:"5 FPS"}),(0,s.jsx)(n.td,{children:"60 FPS"}),(0,s.jsx)(n.td,{children:"12x"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Pose Estimation"}),(0,s.jsx)(n.td,{children:"3 FPS"}),(0,s.jsx)(n.td,{children:"30 FPS"}),(0,s.jsx)(n.td,{children:"10x"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Map Building"}),(0,s.jsx)(n.td,{children:"2 FPS"}),(0,s.jsx)(n.td,{children:"20 FPS"}),(0,s.jsx)(n.td,{children:"10x"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Loop Closure"}),(0,s.jsx)(n.td,{children:"1 FPS"}),(0,s.jsx)(n.td,{children:"15 FPS"}),(0,s.jsx)(n.td,{children:"15x"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"isaac-ros-sensor-processing",children:"Isaac ROS Sensor Processing"}),"\n",(0,s.jsx)(n.h3,{id:"camera-integration",children:"Camera Integration"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS provides optimized processing for various camera types:"}),"\n",(0,s.jsx)(n.h4,{id:"monocular-cameras",children:"Monocular Cameras"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Depth estimation"}),": Mono depth estimation using deep learning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pose tracking"}),": Visual odometry for monocular systems"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scale recovery"}),": Scale-aware processing for metric accuracy"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"stereo-cameras",children:"Stereo Cameras"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Disparity computation"}),": Hardware-accelerated stereo matching"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Depth map generation"}),": Real-time depth map creation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Rectification"}),": Accelerated stereo rectification"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"rgb-d-cameras",children:"RGB-D Cameras"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fusion algorithms"}),": Combining RGB and depth data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Point cloud generation"}),": Real-time 3D point cloud creation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Surface reconstruction"}),": Hardware-accelerated mesh generation"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"lidar-integration",children:"LIDAR Integration"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS also supports accelerated LIDAR processing:"}),"\n",(0,s.jsx)(n.h4,{id:"point-cloud-processing",children:"Point Cloud Processing"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ground plane removal"}),": GPU-accelerated ground segmentation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Clustering algorithms"}),": Accelerated object clustering"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Registration"}),": Hardware-accelerated scan matching"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"obstacle-detection",children:"Obstacle Detection"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Free space computation"}),": Real-time free space mapping"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Obstacle classification"}),": GPU-accelerated object classification"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Path planning integration"}),": Accelerated collision checking"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"practical-examples-isaac-ros-perception-implementation",children:"Practical Examples: Isaac ROS Perception Implementation"}),"\n",(0,s.jsx)(n.h3,{id:"example-1-accelerated-image-pipeline",children:"Example 1: Accelerated Image Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Isaac ROS accelerated image processing pipeline\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\nimport numpy as np\r\n\r\nclass IsaacROSImageProcessor(Node):\r\n    def __init__(self):\r\n        super().__init__(\'isaac_ros_image_processor\')\r\n\r\n        # Subscribe to camera topic\r\n        self.subscription = self.create_subscription(\r\n            Image,\r\n            \'/camera/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher for processed images\r\n        self.publisher = self.create_publisher(\r\n            Image,\r\n            \'/camera/image_processed\',\r\n            10\r\n        )\r\n\r\n        self.bridge = CvBridge()\r\n\r\n        # Isaac ROS specific optimizations\r\n        self.setup_hardware_acceleration()\r\n\r\n    def setup_hardware_acceleration(self):\r\n        """\r\n        Configure GPU acceleration for image processing\r\n        """\r\n        # Initialize CUDA context\r\n        import pycuda.driver as cuda\r\n        cuda.init()\r\n        self.cuda_device = cuda.Device(0)\r\n        self.cuda_context = self.cuda_device.make_context()\r\n\r\n        # Initialize TensorRT engine if available\r\n        try:\r\n            import tensorrt as trt\r\n            self.trt_engine = self.load_tensorrt_model()\r\n        except ImportError:\r\n            self.get_logger().info("TensorRT not available, using CUDA kernels")\r\n            self.trt_engine = None\r\n\r\n    def image_callback(self, msg):\r\n        """\r\n        Process incoming image with hardware acceleration\r\n        """\r\n        # Convert ROS image to OpenCV format\r\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\r\n\r\n        # Process image using GPU acceleration\r\n        processed_image = self.accelerated_process(cv_image)\r\n\r\n        # Publish processed image\r\n        processed_msg = self.bridge.cv2_to_imgmsg(processed_image, encoding=\'bgr8\')\r\n        self.publisher.publish(processed_msg)\r\n\r\n    def accelerated_process(self, image):\r\n        """\r\n        GPU-accelerated image processing function\r\n        """\r\n        # Example: Hardware-accelerated color space conversion\r\n        import cupy as cp\r\n\r\n        # Transfer image to GPU\r\n        gpu_image = cp.asarray(image)\r\n\r\n        # Perform operations on GPU\r\n        # Example: Brightness adjustment\r\n        adjusted_image = gpu_image * 1.2  # Simple brightness boost\r\n\r\n        # Ensure values remain in valid range\r\n        adjusted_image = cp.clip(adjusted_image, 0, 255)\r\n\r\n        # Transfer back to CPU\r\n        result = cp.asnumpy(adjusted_image.astype(np.uint8))\r\n\r\n        return result\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n\r\n    image_processor = IsaacROSImageProcessor()\r\n\r\n    try:\r\n        rclpy.spin(image_processor)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        image_processor.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"example-2-isaac-ros-vslam-node",children:"Example 2: Isaac ROS VSLAM Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Isaac ROS VSLAM implementation\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom nav_msgs.msg import Odometry\r\nimport numpy as np\r\nimport cv2\r\n\r\nclass IsaacROSVSLAMNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'isaac_ros_vslam\')\r\n\r\n        # Subscriptions for stereo camera pair\r\n        self.left_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/left/image_rect\',\r\n            self.left_image_callback,\r\n            10\r\n        )\r\n\r\n        self.right_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/right/image_rect\',\r\n            self.right_image_callback,\r\n            10\r\n        )\r\n\r\n        # Publishers for pose and map\r\n        self.pose_publisher = self.create_publisher(PoseStamped, \'/vslam/pose\', 10)\r\n        self.odom_publisher = self.create_publisher(Odometry, \'/vslam/odometry\', 10)\r\n\r\n        # Camera calibration parameters\r\n        self.camera_matrix_left = None\r\n        self.camera_matrix_right = None\r\n        self.dist_coeffs_left = None\r\n        self.dist_coeffs_right = None\r\n\r\n        # VSLAM state\r\n        self.keyframes = []\r\n        self.current_pose = np.eye(4)  # 4x4 transformation matrix\r\n        self.feature_detector = self.initialize_gpu_feature_detector()\r\n\r\n        # Timing for performance measurement\r\n        self.processing_times = []\r\n\r\n    def initialize_gpu_feature_detector(self):\r\n        """\r\n        Initialize GPU-accelerated feature detector\r\n        """\r\n        try:\r\n            # Use OpenCV\'s CUDA feature detectors if available\r\n            import cv2.cuda as cv2cuda\r\n\r\n            # Create CUDA SURF detector (example)\r\n            detector = cv2cuda.SURF_create(400)\r\n            self.use_cuda_features = True\r\n            self.get_logger().info("Initialized CUDA-based feature detector")\r\n        except AttributeError:\r\n            # Fallback to CPU-based detector with GPU acceleration hints\r\n            detector = cv2.SIFT_create()\r\n            self.use_cuda_features = False\r\n            self.get_logger().info("Using CPU-based feature detector")\r\n\r\n        return detector\r\n\r\n    def left_image_callback(self, msg):\r\n        """\r\n        Process left camera image for VSLAM\r\n        """\r\n        self.process_stereo_pair(msg, self.last_right_image)\r\n\r\n    def right_image_callback(self, msg):\r\n        """\r\n        Process right camera image for VSLAM\r\n        """\r\n        self.last_right_image = msg\r\n\r\n    def process_stereo_pair(self, left_msg, right_msg):\r\n        """\r\n        Process stereo image pair for depth and pose estimation\r\n        """\r\n        if right_msg is None:\r\n            return\r\n\r\n        # Convert ROS images to OpenCV\r\n        bridge = CvBridge()\r\n        left_cv = bridge.imgmsg_to_cv2(left_msg, desired_encoding=\'mono8\')\r\n        right_cv = bridge.imgmsg_to_cv2(right_msg, desired_encoding=\'mono8\')\r\n\r\n        start_time = self.get_clock().now()\r\n\r\n        # Extract features using hardware acceleration\r\n        if self.use_cuda_features:\r\n            left_gpu = cv2.cuda_GpuMat()\r\n            left_gpu.upload(left_cv)\r\n\r\n            keypoints_gpu, descriptors_gpu = self.feature_detector.detectAndCompute(left_gpu, None)\r\n            keypoints = keypoints_gpu.download()\r\n        else:\r\n            keypoints, descriptors = self.feature_detector.detectAndCompute(left_cv, None)\r\n\r\n        # Calculate processing time\r\n        end_time = self.get_clock().now()\r\n        processing_time = (end_time - start_time).nanoseconds / 1e9\r\n        self.processing_times.append(processing_time)\r\n\r\n        # Log performance if needed\r\n        if len(self.processing_times) % 30 == 0:  # Every 30 frames\r\n            avg_time = sum(self.processing_times[-30:]) / 30\r\n            fps = 1.0 / avg_time if avg_time > 0 else 0\r\n            self.get_logger().info(f"VSLAM processing: {fps:.2f} FPS, avg time: {avg_time:.4f}s")\r\n\r\n        # Perform stereo matching and pose estimation\r\n        pose_update = self.compute_stereo_pose(left_cv, right_cv, keypoints)\r\n\r\n        # Update current pose\r\n        if pose_update is not None:\r\n            self.current_pose = self.current_pose @ pose_update\r\n\r\n            # Publish pose\r\n            self.publish_pose()\r\n\r\n    def compute_stereo_pose(self, left_img, right_img, keypoints):\r\n        """\r\n        Compute camera pose using stereo vision\r\n        """\r\n        # Create stereo matcher\r\n        stereo = cv2.StereoBM_create(numDisparities=16, blockSize=15)\r\n\r\n        # Compute disparity\r\n        disparity = stereo.compute(left_img, right_img)\r\n\r\n        # Convert disparity to depth\r\n        baseline = 0.1  # Example baseline (meters)\r\n        focal_length = 525  # Example focal length (pixels)\r\n        depth_map = (baseline * focal_length) / (disparity + 1e-6)\r\n\r\n        # Perform pose estimation based on feature tracking\r\n        # This is a simplified example - real VSLAM would be more complex\r\n        return np.eye(4)  # Identity matrix as placeholder\r\n\r\n    def publish_pose(self):\r\n        """\r\n        Publish current estimated pose\r\n        """\r\n        pose_msg = PoseStamped()\r\n        pose_msg.header.stamp = self.get_clock().now().to_msg()\r\n        pose_msg.header.frame_id = \'map\'\r\n\r\n        # Extract position and orientation from transformation matrix\r\n        position = self.current_pose[:3, 3]\r\n        orientation = self.rotation_matrix_to_quaternion(self.current_pose[:3, :3])\r\n\r\n        pose_msg.pose.position.x = float(position[0])\r\n        pose_msg.pose.position.y = float(position[1])\r\n        pose_msg.pose.position.z = float(position[2])\r\n\r\n        pose_msg.pose.orientation.x = float(orientation[0])\r\n        pose_msg.pose.orientation.y = float(orientation[1])\r\n        pose_msg.pose.orientation.z = float(orientation[2])\r\n        pose_msg.pose.orientation.w = float(orientation[3])\r\n\r\n        self.pose_publisher.publish(pose_msg)\r\n\r\n    def rotation_matrix_to_quaternion(self, R):\r\n        """\r\n        Convert rotation matrix to quaternion\r\n        """\r\n        # Implementation of rotation matrix to quaternion conversion\r\n        trace = np.trace(R)\r\n        if trace > 0:\r\n            s = np.sqrt(trace + 1.0) * 2  # s = 4 * qw\r\n            qw = 0.25 * s\r\n            qx = (R[2, 1] - R[1, 2]) / s\r\n            qy = (R[0, 2] - R[2, 0]) / s\r\n            qz = (R[1, 0] - R[0, 1]) / s\r\n        else:\r\n            if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:\r\n                s = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2\r\n                qw = (R[2, 1] - R[1, 2]) / s\r\n                qx = 0.25 * s\r\n                qy = (R[0, 1] + R[1, 0]) / s\r\n                qz = (R[0, 2] + R[2, 0]) / s\r\n            elif R[1, 1] > R[2, 2]:\r\n                s = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2\r\n                qw = (R[0, 2] - R[2, 0]) / s\r\n                qx = (R[0, 1] + R[1, 0]) / s\r\n                qy = 0.25 * s\r\n                qz = (R[1, 2] + R[2, 1]) / s\r\n            else:\r\n                s = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2\r\n                qw = (R[1, 0] - R[0, 1]) / s\r\n                qx = (R[0, 2] + R[2, 0]) / s\r\n                qy = (R[1, 2] + R[2, 1]) / s\r\n                qz = 0.25 * s\r\n\r\n        return np.array([qx, qy, qz, qw])\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n\r\n    vslam_node = IsaacROSVSLAMNode()\r\n\r\n    try:\r\n        rclpy.spin(vslam_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        vslam_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"example-3-isaac-ros-deep-learning-pipeline",children:"Example 3: Isaac ROS Deep Learning Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Isaac ROS deep learning inference pipeline\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\r\nimport numpy as np\r\n\r\nclass IsaacROSDetectionNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'isaac_ros_detection\')\r\n\r\n        # Subscription to camera\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher for detections\r\n        self.detection_pub = self.create_publisher(\r\n            Detection2DArray,\r\n            \'/isaac_ros/detections\',\r\n            10\r\n        )\r\n\r\n        # Initialize TensorRT engine for inference\r\n        self.tensorrt_engine = self.initialize_tensorrt_engine()\r\n\r\n        # Class labels for the model\r\n        self.class_labels = [\r\n            "person", "bicycle", "car", "motorcycle", "airplane",\r\n            "bus", "train", "truck", "boat", "traffic light",\r\n            "fire hydrant", "stop sign", "parking meter", "bench",\r\n            "bird", "cat", "dog", "horse", "sheep", "cow",\r\n            "elephant", "bear", "zebra", "giraffe", "backpack",\r\n            "umbrella", "handbag", "tie", "suitcase", "frisbee",\r\n            "skis", "snowboard", "sports ball", "kite", "baseball bat",\r\n            "baseball glove", "skateboard", "surfboard", "tennis racket",\r\n            "bottle", "wine glass", "cup", "fork", "knife", "spoon",\r\n            "bowl", "banana", "apple", "sandwich", "orange", "broccoli",\r\n            "carrot", "hot dog", "pizza", "donut", "cake", "chair",\r\n            "couch", "potted plant", "bed", "dining table", "toilet",\r\n            "tv", "laptop", "mouse", "remote", "keyboard", "cell phone",\r\n            "microwave", "oven", "toaster", "sink", "refrigerator",\r\n            "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"\r\n        ]\r\n\r\n    def initialize_tensorrt_engine(self):\r\n        """\r\n        Initialize TensorRT engine for accelerated inference\r\n        """\r\n        try:\r\n            import tensorrt as trt\r\n            import pycuda.driver as cuda\r\n            import pycuda.autoinit\r\n\r\n            # Create logger\r\n            TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\r\n\r\n            # Load pre-built TensorRT engine\r\n            # In a real implementation, you would load a serialized engine file\r\n            engine_file = "/path/to/yolo_v7.engine"  # Example engine file\r\n\r\n            if os.path.exists(engine_file):\r\n                with open(engine_file, \'rb\') as f:\r\n                    engine_data = f.read()\r\n\r\n                runtime = trt.Runtime(TRT_LOGGER)\r\n                engine = runtime.deserialize_cuda_engine(engine_data)\r\n\r\n                self.get_logger().info("TensorRT engine loaded successfully")\r\n                return engine\r\n            else:\r\n                self.get_logger().warn(f"Engine file {engine_file} not found")\r\n                return None\r\n\r\n        except ImportError:\r\n            self.get_logger().warn("TensorRT not available, using alternative inference")\r\n            return None\r\n\r\n    def image_callback(self, msg):\r\n        """\r\n        Process image for object detection using accelerated inference\r\n        """\r\n        # Convert ROS image to numpy array\r\n        bridge = CvBridge()\r\n        cv_image = bridge.imgmsg_to_cv2(msg, desired_encoding=\'bgr8\')\r\n\r\n        # Perform object detection\r\n        if self.tensorrt_engine:\r\n            detections = self.tensorrt_inference(cv_image)\r\n        else:\r\n            # Fallback to other inference method\r\n            detections = self.fallback_inference(cv_image)\r\n\r\n        # Publish detections\r\n        self.publish_detections(detections, msg.header)\r\n\r\n    def tensorrt_inference(self, image):\r\n        """\r\n        Perform inference using TensorRT engine\r\n        """\r\n        # Resize image to model input size\r\n        input_height, input_width = 640, 640  # Example model input size\r\n        resized_image = cv2.resize(image, (input_width, input_height))\r\n\r\n        # Normalize image\r\n        normalized_image = resized_image.astype(np.float32) / 255.0\r\n        normalized_image = np.transpose(normalized_image, (2, 0, 1))  # HWC to CHW\r\n        normalized_image = np.expand_dims(normalized_image, axis=0)  # Add batch dimension\r\n\r\n        # Perform inference using TensorRT\r\n        # This is a simplified example - actual implementation would be more complex\r\n        # involving CUDA memory management and engine execution\r\n\r\n        # Placeholder for actual TensorRT inference\r\n        # In reality, you would:\r\n        # 1. Allocate CUDA memory\r\n        # 2. Copy input data to GPU\r\n        # 3. Execute TensorRT engine\r\n        # 4. Copy results back from GPU\r\n        # 5. Process outputs\r\n\r\n        # For demonstration, returning dummy detections\r\n        dummy_detections = [\r\n            {\r\n                \'class_id\': 0,\r\n                \'confidence\': 0.85,\r\n                \'bbox\': {\'xmin\': 100, \'ymin\': 100, \'xmax\': 200, \'ymax\': 200}\r\n            }\r\n        ]\r\n\r\n        return dummy_detections\r\n\r\n    def fallback_inference(self, image):\r\n        """\r\n        Fallback inference method if TensorRT is not available\r\n        """\r\n        # This would typically use a CPU-based inference engine\r\n        # or a different acceleration method\r\n        self.get_logger().warn("Using fallback inference method")\r\n        return []  # Return empty list as placeholder\r\n\r\n    def publish_detections(self, detections, header):\r\n        """\r\n        Publish detection results to ROS topic\r\n        """\r\n        detection_array = Detection2DArray()\r\n        detection_array.header = header\r\n\r\n        for det in detections:\r\n            detection_msg = Detection2D()\r\n            detection_msg.header = header\r\n\r\n            # Set bounding box\r\n            detection_msg.bbox.size_x = det[\'bbox\'][\'xmax\'] - det[\'bbox\'][\'xmin\']\r\n            detection_msg.bbox.size_y = det[\'bbox\'][\'ymax\'] - det[\'bbox\'][\'ymin\']\r\n            detection_msg.bbox.center.x = det[\'bbox\'][\'xmin\'] + detection_msg.bbox.size_x / 2\r\n            detection_msg.bbox.center.y = det[\'bbox\'][\'ymin\'] + detection_msg.bbox.size_y / 2\r\n\r\n            # Set hypothesis\r\n            hypothesis = ObjectHypothesisWithPose()\r\n            hypothesis.id = str(det[\'class_id\'])\r\n            hypothesis.score = det[\'confidence\']\r\n\r\n            detection_msg.results.append(hypothesis)\r\n\r\n        self.detection_pub.publish(detection_array)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n\r\n    detection_node = IsaacROSDetectionNode()\r\n\r\n    try:\r\n        rclpy.spin(detection_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        detection_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"configuration-examples-for-isaac-ros-pipelines",children:"Configuration Examples for Isaac ROS Pipelines"}),"\n",(0,s.jsx)(n.h3,{id:"1-isaac-ros-launch-configuration",children:"1. Isaac ROS Launch Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'\x3c!-- isaac_ros_pipeline.launch.xml --\x3e\r\n<launch>\r\n  \x3c!-- Arguments --\x3e\r\n  <arg name="camera_namespace" default="/camera"/>\r\n  <arg name="image_topic" default="image_raw"/>\r\n  <arg name="compressed" default="false"/>\r\n\r\n  \x3c!-- Isaac ROS Image Pipeline --\x3e\r\n  <group>\r\n    <node pkg="isaac_ros_image_pipeline" exec="isaac_ros_image_decoder" name="image_decoder">\r\n      <param name="input_encoding" value="rgb8"/>\r\n      <param name="output_encoding" value="rgba8"/>\r\n    </node>\r\n\r\n    <node pkg="isaac_ros_image_pipeline" exec="isaac_ros_rectifier" name="rectifier">\r\n      <param name="input_camera_info_topic" value="$(var camera_namespace)/camera_info"/>\r\n      <param name="output_camera_info_topic" value="$(var camera_namespace)/rectified/camera_info"/>\r\n    </node>\r\n  </group>\r\n\r\n  \x3c!-- Isaac ROS VSLAM Node --\x3e\r\n  <node pkg="isaac_ros_visual_slam" exec="isaac_ros_visual_slam_node" name="visual_slam">\r\n    <param name="enable_imu" value="true"/>\r\n    <param name="use_sim_time" value="false"/>\r\n    <param name="map_frame" value="map"/>\r\n    <param name="odom_frame" value="odom"/>\r\n    <param name="base_frame" value="base_link"/>\r\n    <param name="publish_odom_tf" value="true"/>\r\n  </node>\r\n\r\n  \x3c!-- Isaac ROS Detection Node --\x3e\r\n  <node pkg="isaac_ros_detectnet" exec="isaac_ros_detectnet" name="detectnet">\r\n    <param name="input_topic" value="$(var camera_namespace)/image_rect_color"/>\r\n    <param name="network_type" value="detectnet"/>\r\n    <param name="model_name" value="ssd-mobilenet-v2"/>\r\n    <param name="input_layer" value="image"/>\r\n    <param name="output_layer" value="detections"/>\r\n    <param name="threshold" value="0.5"/>\r\n  </node>\r\n</launch>\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-isaac-ros-parameter-configuration",children:"2. Isaac ROS Parameter Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# isaac_ros_params.yaml\r\n/**:\r\n  ros__parameters:\r\n    use_sim_time: false\r\n\r\n# Isaac ROS Image Pipeline Parameters\r\nisaac_ros_image_decoder:\r\n  ros__parameters:\r\n    input_encoding: "rgb8"\r\n    output_encoding: "rgba8"\r\n    queue_size: 1\r\n    enable_debug_mode: false\r\n\r\n# Isaac ROS Visual SLAM Parameters\r\nisaac_ros_visual_slam:\r\n  ros__parameters:\r\n    enable_imu: true\r\n    imu_topic: "/imu/data"\r\n    camera_topic: "/camera/image_rect_color"\r\n    camera_info_topic: "/camera/camera_info"\r\n    map_frame: "map"\r\n    odom_frame: "odom"\r\n    base_frame: "base_link"\r\n    publish_odom_tf: true\r\n    publish_map_tf: true\r\n    min_num_features: 100\r\n    max_num_features: 1000\r\n    feature_detector_type: "SHI_TOMASI"\r\n    descriptor_extractor_type: "ORB"\r\n    matching_strategy: "BRUTE_FORCE_HAMMING"\r\n    enable_localization: true\r\n    enable_mapping: true\r\n\r\n# Isaac ROS Detection Parameters\r\nisaac_ros_detectnet:\r\n  ros__parameters:\r\n    input_topic: "/camera/image_rect_color"\r\n    output_topic: "/detectnet/detections"\r\n    model_name: "ssd-mobilenet-v2"\r\n    input_layer: "image"\r\n    output_layer: "detections"\r\n    threshold: 0.5\r\n    publish_objects_masks: false\r\n    publish_segmentation_masks: false\r\n    mask_topic: "/detectnet/masks"\n'})}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting-isaac-ros-integration",children:"Troubleshooting Isaac ROS Integration"}),"\n",(0,s.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,s.jsx)(n.h4,{id:"1-gpu-memory-issues",children:"1. GPU Memory Issues"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),': "Out of memory" errors during inference']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cause"}),": Insufficient GPU memory for model or batch size"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Solution"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Reduce batch size in inference configuration"}),"\n",(0,s.jsx)(n.li,{children:"Use model quantization to reduce memory footprint"}),"\n",(0,s.jsxs)(n.li,{children:["Monitor GPU memory usage with ",(0,s.jsx)(n.code,{children:"nvidia-smi"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"2-cuda-compatibility-issues",children:"2. CUDA Compatibility Issues"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),': "CUDA error" during initialization']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cause"}),": CUDA version mismatch or unsupported GPU"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Solution"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Verify CUDA version compatibility with Isaac ROS"}),"\n",(0,s.jsx)(n.li,{children:"Check GPU compute capability requirements"}),"\n",(0,s.jsx)(n.li,{children:"Update GPU drivers to latest version"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"3-performance-bottlenecks",children:"3. Performance Bottlenecks"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Low frame rates or high latency"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cause"}),": CPU-GPU synchronization or inefficient pipeline"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Solution"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Optimize data transfers between CPU and GPU"}),"\n",(0,s.jsx)(n.li,{children:"Use asynchronous processing where possible"}),"\n",(0,s.jsx)(n.li,{children:"Profile pipeline to identify bottlenecks"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"4-calibration-issues",children:"4. Calibration Issues"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Problem"}),": Incorrect depth maps or pose estimates"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cause"}),": Improper camera calibration or stereo rectification"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Solution"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Recalibrate cameras using standard ROS calibration tools"}),"\n",(0,s.jsx)(n.li,{children:"Verify stereo rectification parameters"}),"\n",(0,s.jsx)(n.li,{children:"Check baseline and focal length values"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"performance-optimization-tips",children:"Performance Optimization Tips"}),"\n",(0,s.jsx)(n.h4,{id:"1-pipeline-optimization",children:"1. Pipeline Optimization"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Use hardware buffers"}),": Minimize CPU-GPU transfers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pipeline parallelism"}),": Process multiple frames in parallel"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory pooling"}),": Reuse GPU memory allocations"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"2-model-optimization",children:"2. Model Optimization"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"TensorRT optimization"}),": Convert models to TensorRT format"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Quantization"}),": Use INT8 or FP16 precision where possible"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model pruning"}),": Remove unnecessary layers or parameters"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"3-resource-management",children:"3. Resource Management"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU scheduling"}),": Use appropriate CUDA stream priorities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory management"}),": Implement proper allocation/deallocation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Thermal management"}),": Monitor GPU temperature and adjust load"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"exercises-for-isaac-ros-integration",children:"Exercises for Isaac ROS Integration"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-1-basic-isaac-ros-pipeline-setup",children:"Exercise 1: Basic Isaac ROS Pipeline Setup"}),"\n",(0,s.jsx)(n.p,{children:"Implement a basic Isaac ROS pipeline that processes camera images with hardware acceleration."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Set up Isaac ROS image decoder node"}),"\n",(0,s.jsx)(n.li,{children:"Configure hardware-accelerated image processing"}),"\n",(0,s.jsx)(n.li,{children:"Verify pipeline performance improvement over CPU-only processing"}),"\n",(0,s.jsx)(n.li,{children:"Measure and compare processing times"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Steps:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Create a new ROS 2 package for your Isaac ROS pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Install Isaac ROS image pipeline packages"}),"\n",(0,s.jsx)(n.li,{children:"Configure the image decoder with appropriate encoding settings"}),"\n",(0,s.jsx)(n.li,{children:"Launch the pipeline and verify image processing"}),"\n",(0,s.jsx)(n.li,{children:"Benchmark performance against CPU-only processing"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Solution Outline:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create workspace\r\nmkdir -p ~/isaac_ros_ws/src\r\ncd ~/isaac_ros_ws\r\n\r\n# Install Isaac ROS image pipeline\r\nsudo apt update\r\nsudo apt install ros-humble-isaac-ros-image-pipeline\r\n\r\n# Build workspace\r\ncolcon build --packages-select your_package_name\r\nsource install/setup.bash\r\n\r\n# Launch pipeline\r\nros2 launch your_package_name basic_pipeline.launch.py\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Expected Output:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Images processed at higher frame rates compared to CPU processing"}),"\n",(0,s.jsx)(n.li,{children:"GPU utilization visible in nvidia-smi"}),"\n",(0,s.jsx)(n.li,{children:"Properly decoded and rectified image topics"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-2-vslam-integration",children:"Exercise 2: VSLAM Integration"}),"\n",(0,s.jsx)(n.p,{children:"Integrate Isaac ROS VSLAM with your robot platform."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Configure Isaac ROS Visual SLAM node"}),"\n",(0,s.jsx)(n.li,{children:"Integrate with robot's camera and IMU sensors"}),"\n",(0,s.jsx)(n.li,{children:"Verify accurate pose estimation and map building"}),"\n",(0,s.jsx)(n.li,{children:"Test performance in different environments"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Steps:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Install Isaac ROS Visual SLAM packages"}),"\n",(0,s.jsx)(n.li,{children:"Calibrate stereo cameras or RGB-D sensor"}),"\n",(0,s.jsx)(n.li,{children:"Configure VSLAM parameters for your robot"}),"\n",(0,s.jsx)(n.li,{children:"Test SLAM performance in different environments"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate map quality and pose accuracy"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Configuration Parameters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Feature detector type (ORB, SIFT, etc.)"}),"\n",(0,s.jsx)(n.li,{children:"Tracking parameters (keyframe threshold, etc.)"}),"\n",(0,s.jsx)(n.li,{children:"Map building parameters (localization vs mapping mode)"}),"\n",(0,s.jsx)(n.li,{children:"IMU integration settings (if available)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Validation Metrics:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Tracking accuracy (RMSE of pose estimates)"}),"\n",(0,s.jsx)(n.li,{children:"Frame rate maintenance (should be real-time)"}),"\n",(0,s.jsx)(n.li,{children:"Map consistency (loop closure detection)"}),"\n",(0,s.jsx)(n.li,{children:"Computational efficiency (GPU utilization)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-3-accelerated-object-detection",children:"Exercise 3: Accelerated Object Detection"}),"\n",(0,s.jsx)(n.p,{children:"Implement accelerated object detection using Isaac ROS detectnet."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Set up Isaac ROS detectnet node"}),"\n",(0,s.jsx)(n.li,{children:"Configure for real-time inference on your hardware"}),"\n",(0,s.jsx)(n.li,{children:"Integrate detection results with robot navigation"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate detection accuracy and performance"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Steps:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Install Isaac ROS detection packages"}),"\n",(0,s.jsx)(n.li,{children:"Select appropriate pre-trained model for your use case"}),"\n",(0,s.jsx)(n.li,{children:"Configure detection parameters (confidence threshold, etc.)"}),"\n",(0,s.jsx)(n.li,{children:"Integrate detection results into perception pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Benchmark performance against CPU-based detection"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Model Selection:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Choose between SSD, YOLO, or other available models"}),"\n",(0,s.jsx)(n.li,{children:"Consider trade-offs between accuracy and speed"}),"\n",(0,s.jsx)(n.li,{children:"Optimize for your specific object classes"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Performance Metrics:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Frames per second (FPS) achieved"}),"\n",(0,s.jsx)(n.li,{children:"Detection accuracy (mAP score)"}),"\n",(0,s.jsx)(n.li,{children:"GPU memory utilization"}),"\n",(0,s.jsx)(n.li,{children:"Power consumption comparison"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-4-multi-sensor-fusion",children:"Exercise 4: Multi-Sensor Fusion"}),"\n",(0,s.jsx)(n.p,{children:"Combine multiple Isaac ROS perception nodes."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate camera, LIDAR, and IMU processing"}),"\n",(0,s.jsx)(n.li,{children:"Fuse sensor data for robust perception"}),"\n",(0,s.jsx)(n.li,{children:"Handle timing and synchronization"}),"\n",(0,s.jsx)(n.li,{children:"Validate fused output quality"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Steps:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up individual perception nodes"}),"\n",(0,s.jsx)(n.li,{children:"Configure message synchronization"}),"\n",(0,s.jsx)(n.li,{children:"Implement sensor fusion algorithm"}),"\n",(0,s.jsx)(n.li,{children:"Validate fused output quality"}),"\n",(0,s.jsx)(n.li,{children:"Optimize for computational efficiency"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Synchronization Challenges:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Different sensor frequencies"}),"\n",(0,s.jsx)(n.li,{children:"Latency compensation"}),"\n",(0,s.jsx)(n.li,{children:"Time stamp alignment"}),"\n",(0,s.jsx)(n.li,{children:"Buffer management"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-5-custom-perception-pipeline",children:"Exercise 5: Custom Perception Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"Design and implement a custom perception pipeline for a specific robot application."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Identify specific perception requirements for your robot"}),"\n",(0,s.jsx)(n.li,{children:"Design pipeline architecture considering hardware constraints"}),"\n",(0,s.jsx)(n.li,{children:"Implement and test pipeline components"}),"\n",(0,s.jsx)(n.li,{children:"Optimize for your specific use case"}),"\n",(0,s.jsx)(n.li,{children:"Document performance characteristics"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Application Scenarios:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Indoor navigation with obstacle avoidance"}),"\n",(0,s.jsx)(n.li,{children:"Object manipulation with grasp planning"}),"\n",(0,s.jsx)(n.li,{children:"Person following with social navigation"}),"\n",(0,s.jsx)(n.li,{children:"Warehouse inspection with anomaly detection"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Design Considerations:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Computational budget allocation"}),"\n",(0,s.jsx)(n.li,{children:"Real-time performance requirements"}),"\n",(0,s.jsx)(n.li,{children:"Accuracy vs. speed trade-offs"}),"\n",(0,s.jsx)(n.li,{children:"Robustness to environmental conditions"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-6-performance-optimization",children:"Exercise 6: Performance Optimization"}),"\n",(0,s.jsx)(n.p,{children:"Optimize an Isaac ROS pipeline for maximum performance."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Profile current pipeline performance"}),"\n",(0,s.jsx)(n.li,{children:"Identify bottlenecks and inefficiencies"}),"\n",(0,s.jsx)(n.li,{children:"Apply optimization techniques"}),"\n",(0,s.jsx)(n.li,{children:"Measure performance improvements"}),"\n",(0,s.jsx)(n.li,{children:"Document optimization strategies"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Optimization Techniques:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Memory management improvements"}),"\n",(0,s.jsx)(n.li,{children:"CUDA kernel optimizations"}),"\n",(0,s.jsx)(n.li,{children:"Pipeline parallelization"}),"\n",(0,s.jsx)(n.li,{children:"Model quantization"}),"\n",(0,s.jsx)(n.li,{children:"Batch size optimization"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-7-isaac-ros-deployment",children:"Exercise 7: Isaac ROS Deployment"}),"\n",(0,s.jsx)(n.p,{children:"Deploy an Isaac ROS pipeline on a target robot platform."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Requirements:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Prepare target platform with Isaac ROS dependencies"}),"\n",(0,s.jsx)(n.li,{children:"Optimize pipeline for target hardware constraints"}),"\n",(0,s.jsx)(n.li,{children:"Test deployment in real-world scenarios"}),"\n",(0,s.jsx)(n.li,{children:"Validate performance on target platform"}),"\n",(0,s.jsx)(n.li,{children:"Document deployment procedures"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Deployment Considerations:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Hardware compatibility verification"}),"\n",(0,s.jsx)(n.li,{children:"Thermal management"}),"\n",(0,s.jsx)(n.li,{children:"Power consumption optimization"}),"\n",(0,s.jsx)(n.li,{children:"Real-time performance validation"}),"\n",(0,s.jsx)(n.li,{children:"Safety and fault tolerance"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS provides powerful GPU-accelerated perception capabilities that significantly enhance robotic applications. By leveraging NVIDIA's hardware acceleration, developers can achieve real-time performance for complex AI-powered perception tasks that would otherwise be computationally prohibitive. The integration with ROS 2 ensures compatibility with existing robotic systems while providing substantial performance improvements."}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"Continue with the next topics in Module 3:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/module-3/chapter-1-isaac-sim-essentials",children:"Chapter 1: Isaac Sim Essentials"})," - Review simulation fundamentals"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/module-3/chapter-3-nav2-humanoid-navigation",children:"Chapter 3: Nav2 for Humanoid Navigation"})," - Explore navigation solutions for humanoid robots"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Or explore other modules:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/module-1/chapter-1-ros2-fundamentals",children:"Module 1: The Robotic Nervous System (ROS 2)"})," - Fundamentals of ROS 2"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/module-2/chapter-1-gazebo-basics",children:"Module 2: The Digital Twin (Gazebo & Unity)"})," - Simulation and interaction concepts"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);