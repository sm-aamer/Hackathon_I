"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[2784],{2570(n,e,i){i.r(e),i.d(e,{assets:()=>r,contentTitle:()=>l,default:()=>d,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4/intro","title":"Introduction to Vision-Language-Action (VLA)","description":"Introduction to Vision-Language-Action systems for humanoid robotics, covering voice-to-action pipelines, cognitive planning with LLMs, and autonomous humanoid systems.","source":"@site/docs/module-4/intro.md","sourceDirName":"module-4","slug":"/module-4/intro","permalink":"/docs/module-4/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4/intro.md","tags":[{"inline":true,"label":"vla","permalink":"/docs/tags/vla"},{"inline":true,"label":"vision-language-action","permalink":"/docs/tags/vision-language-action"},{"inline":true,"label":"robotics","permalink":"/docs/tags/robotics"},{"inline":true,"label":"llm","permalink":"/docs/tags/llm"},{"inline":true,"label":"voice-control","permalink":"/docs/tags/voice-control"},{"inline":true,"label":"cognitive-planning","permalink":"/docs/tags/cognitive-planning"}],"version":"current","sidebarPosition":0,"frontMatter":{"title":"Introduction to Vision-Language-Action (VLA)","sidebar_position":0,"tags":["vla","vision-language-action","robotics","llm","voice-control","cognitive-planning"],"description":"Introduction to Vision-Language-Action systems for humanoid robotics, covering voice-to-action pipelines, cognitive planning with LLMs, and autonomous humanoid systems."},"sidebar":"tutorialSidebar","previous":{"title":"Nav2 for Humanoid Navigation","permalink":"/docs/module-3/chapter-3-nav2-humanoid-navigation"},"next":{"title":"VLA Module Quickstart","permalink":"/docs/module-4/quickstart"}}');var t=i(4848),s=i(8453);const a={title:"Introduction to Vision-Language-Action (VLA)",sidebar_position:0,tags:["vla","vision-language-action","robotics","llm","voice-control","cognitive-planning"],description:"Introduction to Vision-Language-Action systems for humanoid robotics, covering voice-to-action pipelines, cognitive planning with LLMs, and autonomous humanoid systems."},l="Introduction to Vision-Language-Action (VLA)",r={},c=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2}];function u(n){const e={h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"introduction-to-vision-language-action-vla",children:"Introduction to Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(e.p,{children:"Welcome to Module 4: Vision-Language-Action (VLA), where you'll learn about LLM-driven robotics and natural-language control. This module focuses on voice-to-action pipelines, cognitive planning with LLMs, and the Autonomous Humanoid capstone project."}),"\n",(0,t.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(e.p,{children:"In this module, you will explore how modern robotics systems integrate vision, language, and action to create intelligent, responsive robots that can understand and execute natural language commands. We'll cover:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Voice-to-Action systems using Whisper API"}),"\n",(0,t.jsx)(e.li,{children:"Cognitive planning with Large Language Models"}),"\n",(0,t.jsx)(e.li,{children:"Autonomous humanoid systems that integrate all components"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(e.p,{children:"Before starting this module, you should have:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Completed Modules 1-3 of the Physical AI & Humanoid Robotics book"}),"\n",(0,t.jsx)(e.li,{children:"Basic understanding of ROS 2 concepts and navigation"}),"\n",(0,t.jsx)(e.li,{children:"Familiarity with simulation environments (Gazebo/Isaac Sim)"}),"\n",(0,t.jsx)(e.li,{children:"Access to a humanoid robot or simulator"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(e.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Implement voice-controlled robot systems"}),"\n",(0,t.jsx)(e.li,{children:"Use LLMs for cognitive planning and task decomposition"}),"\n",(0,t.jsx)(e.li,{children:"Create autonomous humanoid systems that respond to natural language"}),"\n",(0,t.jsx)(e.li,{children:"Integrate vision, language, and action components into cohesive systems"}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(u,{...n})}):u(n)}},8453(n,e,i){i.d(e,{R:()=>a,x:()=>l});var o=i(6540);const t={},s=o.createContext(t);function a(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),o.createElement(s.Provider,{value:e},n.children)}}}]);