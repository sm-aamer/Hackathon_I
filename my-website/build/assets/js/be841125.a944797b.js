"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[5653],{5588(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>u,frontMatter:()=>l,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/quickstart","title":"VLA Module Quickstart","description":"Quickstart guide for the Vision-Language-Action module, covering voice-to-action pipelines, cognitive planning with LLMs, and autonomous humanoid systems.","source":"@site/docs/module-4/quickstart.md","sourceDirName":"module-4","slug":"/module-4/quickstart","permalink":"/docs/module-4/quickstart","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4/quickstart.md","tags":[{"inline":true,"label":"vla","permalink":"/docs/tags/vla"},{"inline":true,"label":"quickstart","permalink":"/docs/tags/quickstart"},{"inline":true,"label":"robotics","permalink":"/docs/tags/robotics"},{"inline":true,"label":"llm","permalink":"/docs/tags/llm"},{"inline":true,"label":"voice-control","permalink":"/docs/tags/voice-control"},{"inline":true,"label":"setup","permalink":"/docs/tags/setup"}],"version":"current","sidebarPosition":1,"frontMatter":{"title":"VLA Module Quickstart","sidebar_position":1,"tags":["vla","quickstart","robotics","llm","voice-control","setup"],"description":"Quickstart guide for the Vision-Language-Action module, covering voice-to-action pipelines, cognitive planning with LLMs, and autonomous humanoid systems."},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to Vision-Language-Action (VLA)","permalink":"/docs/module-4/intro"},"next":{"title":"Voice-to-Action Pipeline","permalink":"/docs/module-4/chapter-1-voice-to-action"}}');var o=i(4848),s=i(8453);const l={title:"VLA Module Quickstart",sidebar_position:1,tags:["vla","quickstart","robotics","llm","voice-control","setup"],description:"Quickstart guide for the Vision-Language-Action module, covering voice-to-action pipelines, cognitive planning with LLMs, and autonomous humanoid systems."},r="Vision-Language-Action (VLA) Module Quickstart",a={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"1. Voice-to-Action Pipeline (Chapter 1)",id:"1-voice-to-action-pipeline-chapter-1",level:3},{value:"2. Cognitive Planning with LLMs (Chapter 2)",id:"2-cognitive-planning-with-llms-chapter-2",level:3},{value:"3. Autonomous Humanoid Capstone (Chapter 3)",id:"3-autonomous-humanoid-capstone-chapter-3",level:3},{value:"Key Technologies",id:"key-technologies",level:2},{value:"Sample Commands",id:"sample-commands",level:2},{value:"Development Environment Setup",id:"development-environment-setup",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Resources",id:"resources",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"vision-language-action-vla-module-quickstart",children:"Vision-Language-Action (VLA) Module Quickstart"})}),"\n",(0,o.jsx)(n.p,{children:"This quickstart guide will help you get up and running with the Vision-Language-Action (VLA) module. This module introduces students to LLM-driven robotics and natural-language control, focusing on voice-to-action pipelines, cognitive planning with LLMs, and the Autonomous Humanoid capstone."}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(n.p,{children:"Before starting this module, you should have:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Completed Modules 1-3 of the Physical AI & Humanoid Robotics book"}),"\n",(0,o.jsx)(n.li,{children:"Basic understanding of ROS 2 concepts and navigation"}),"\n",(0,o.jsx)(n.li,{children:"Familiarity with simulation environments (Gazebo/Isaac Sim)"}),"\n",(0,o.jsx)(n.li,{children:"Access to a humanoid robot or simulator"}),"\n",(0,o.jsx)(n.li,{children:"An OpenAI API key for LLM integration"}),"\n",(0,o.jsx)(n.li,{children:"Microphone access for voice input"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,o.jsx)(n.h3,{id:"1-voice-to-action-pipeline-chapter-1",children:"1. Voice-to-Action Pipeline (Chapter 1)"}),"\n",(0,o.jsx)(n.p,{children:"Begin with the fundamentals of converting speech to robot commands:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Set up your audio input device"}),"\n",(0,o.jsx)(n.li,{children:"Test the Whisper API integration"}),"\n",(0,o.jsx)(n.li,{children:"Learn how natural language commands are interpreted"}),"\n",(0,o.jsx)(n.li,{children:'Practice basic voice commands like "move forward" or "turn left"'}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"2-cognitive-planning-with-llms-chapter-2",children:"2. Cognitive Planning with LLMs (Chapter 2)"}),"\n",(0,o.jsx)(n.p,{children:"Advance to complex command interpretation:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Understand how LLMs decompose complex commands"}),"\n",(0,o.jsx)(n.li,{children:"Learn about action sequencing and planning"}),"\n",(0,o.jsx)(n.li,{children:'Practice multi-step commands like "go to the kitchen and pick up the red cup"'}),"\n",(0,o.jsx)(n.li,{children:"Explore error handling and recovery mechanisms"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"3-autonomous-humanoid-capstone-chapter-3",children:"3. Autonomous Humanoid Capstone (Chapter 3)"}),"\n",(0,o.jsx)(n.p,{children:"Implement the full VLA pipeline:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Integrate all components for end-to-end functionality"}),"\n",(0,o.jsx)(n.li,{children:"Test complex goal-oriented commands"}),"\n",(0,o.jsx)(n.li,{children:"Learn about adaptive planning and environmental awareness"}),"\n",(0,o.jsx)(n.li,{children:"Complete the capstone project of autonomous humanoid operation"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"key-technologies",children:"Key Technologies"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech Recognition"}),": OpenAI Whisper API for converting voice to text"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Natural Language Processing"}),": GPT models for interpreting commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robotics Framework"}),": ROS 2 Humble for robot control"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigation"}),": Nav2 for path planning"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simulation"}),": Isaac Sim for testing environments"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"sample-commands",children:"Sample Commands"}),"\n",(0,o.jsx)(n.p,{children:"Try these sample voice commands to get started:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'"Move forward 1 meter"'}),"\n",(0,o.jsx)(n.li,{children:'"Turn left 90 degrees"'}),"\n",(0,o.jsx)(n.li,{children:'"Go to the table"'}),"\n",(0,o.jsx)(n.li,{children:'"Pick up the red block"'}),"\n",(0,o.jsx)(n.li,{children:'"Bring the cup to the kitchen"'}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"development-environment-setup",children:"Development Environment Setup"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Clone the repository"}),"\n",(0,o.jsx)(n.li,{children:"Install ROS 2 Humble"}),"\n",(0,o.jsx)(n.li,{children:"Set up OpenAI API access"}),"\n",(0,o.jsx)(n.li,{children:"Configure audio input devices"}),"\n",(0,o.jsx)(n.li,{children:"Set up Isaac Sim for testing"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsx)(n.p,{children:"After completing this quickstart:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Proceed to Chapter 1: Voice-to-Action (Whisper)"}),"\n",(0,o.jsx)(n.li,{children:"Continue with Chapter 2: Cognitive Planning with LLMs"}),"\n",(0,o.jsx)(n.li,{children:"Complete the capstone in Chapter 3: Autonomous Humanoid"}),"\n",(0,o.jsx)(n.li,{children:"Integrate with your own robotics projects"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["ROS 2 Documentation: ",(0,o.jsx)(n.a,{href:"https://docs.ros.org/",children:"https://docs.ros.org/"})]}),"\n",(0,o.jsxs)(n.li,{children:["OpenAI API: ",(0,o.jsx)(n.a,{href:"https://platform.openai.com/docs/",children:"https://platform.openai.com/docs/"})]}),"\n",(0,o.jsxs)(n.li,{children:["Isaac Sim: ",(0,o.jsx)(n.a,{href:"https://developer.nvidia.com/isaac-sim",children:"https://developer.nvidia.com/isaac-sim"})]}),"\n",(0,o.jsx)(n.li,{children:"Module 4 Source Code: [To be created during implementation]"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>l,x:()=>r});var t=i(6540);const o={},s=t.createContext(o);function l(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:l(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);