"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[2284],{2066(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-4/chapter-1-voice-to-action","title":"Voice-to-Action Pipeline","description":"Learn how to implement voice-controlled robot systems using Whisper API for speech recognition and mapping natural language commands to ROS 2 actions.","source":"@site/docs/module-4/chapter-1-voice-to-action.md","sourceDirName":"module-4","slug":"/module-4/chapter-1-voice-to-action","permalink":"/docs/module-4/chapter-1-voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4/chapter-1-voice-to-action.md","tags":[{"inline":true,"label":"voice-recognition","permalink":"/docs/tags/voice-recognition"},{"inline":true,"label":"whisper","permalink":"/docs/tags/whisper"},{"inline":true,"label":"speech-to-text","permalink":"/docs/tags/speech-to-text"},{"inline":true,"label":"robotics","permalink":"/docs/tags/robotics"},{"inline":true,"label":"command-interpreter","permalink":"/docs/tags/command-interpreter"},{"inline":true,"label":"ros2-actions","permalink":"/docs/tags/ros-2-actions"}],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Voice-to-Action Pipeline","sidebar_position":2,"tags":["voice-recognition","whisper","speech-to-text","robotics","command-interpreter","ros2-actions"],"description":"Learn how to implement voice-controlled robot systems using Whisper API for speech recognition and mapping natural language commands to ROS 2 actions."},"sidebar":"tutorialSidebar","previous":{"title":"VLA Module Quickstart","permalink":"/docs/module-4/quickstart"},"next":{"title":"Cognitive Planning with LLMs","permalink":"/docs/module-4/chapter-2-cognitive-planning"}}');var o=i(4848),s=i(8453);const r={title:"Voice-to-Action Pipeline",sidebar_position:2,tags:["voice-recognition","whisper","speech-to-text","robotics","command-interpreter","ros2-actions"],description:"Learn how to implement voice-controlled robot systems using Whisper API for speech recognition and mapping natural language commands to ROS 2 actions."},a="Voice-to-Action Pipeline",c={},l=[{value:"Overview of Voice-to-Action Systems",id:"overview-of-voice-to-action-systems",level:2},{value:"Whisper API Integration for Speech Recognition",id:"whisper-api-integration-for-speech-recognition",level:2},{value:"Basic Whisper Integration",id:"basic-whisper-integration",level:3},{value:"Real-time Audio Processing",id:"real-time-audio-processing",level:3},{value:"Natural Language Processing for Command Interpretation",id:"natural-language-processing-for-command-interpretation",level:2},{value:"Intent Classification with LLM",id:"intent-classification-with-llm",level:3},{value:"Mapping Voice Commands to ROS 2 Actions",id:"mapping-voice-commands-to-ros-2-actions",level:2},{value:"ROS 2 Action Client for Navigation",id:"ros-2-action-client-for-navigation",level:3},{value:"Practical Examples of Voice Command Implementations",id:"practical-examples-of-voice-command-implementations",level:2},{value:"Complete Voice Command System",id:"complete-voice-command-system",level:3},{value:"Configuration Examples for Whisper Integration",id:"configuration-examples-for-whisper-integration",level:2},{value:"Environment Configuration",id:"environment-configuration",level:3},{value:"Python Configuration",id:"python-configuration",level:3},{value:"Troubleshooting Voice Recognition Issues",id:"troubleshooting-voice-recognition-issues",level:2},{value:"Common Problems and Solutions",id:"common-problems-and-solutions",level:3},{value:"1. Poor Recognition Quality",id:"1-poor-recognition-quality",level:4},{value:"2. High Latency",id:"2-high-latency",level:4},{value:"3. Incorrect Intent Classification",id:"3-incorrect-intent-classification",level:4},{value:"Performance Optimization Tips",id:"performance-optimization-tips",level:3},{value:"1. Audio Processing Optimization",id:"1-audio-processing-optimization",level:4},{value:"2. API Call Optimization",id:"2-api-call-optimization",level:4},{value:"Exercises for Voice-to-Action Implementation",id:"exercises-for-voice-to-action-implementation",level:2},{value:"Exercise 1: Basic Voice Command Recognition",id:"exercise-1-basic-voice-command-recognition",level:3},{value:"Exercise 2: Context-Aware Command Processing",id:"exercise-2-context-aware-command-processing",level:3},{value:"Exercise 3: Error Handling and Recovery",id:"exercise-3-error-handling-and-recovery",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"voice-to-action-pipeline",children:"Voice-to-Action Pipeline"})}),"\n",(0,o.jsx)(n.p,{children:"This chapter covers the fundamentals of implementing voice-controlled robot systems using the OpenAI Whisper API for speech recognition and mapping natural language commands to ROS 2 actions. This is the foundational capability that enables all other voice-controlled interactions in the Vision-Language-Action (VLA) framework."}),"\n",(0,o.jsx)(n.h2,{id:"overview-of-voice-to-action-systems",children:"Overview of Voice-to-Action Systems"}),"\n",(0,o.jsx)(n.p,{children:"Voice-to-Action systems enable robots to understand and execute natural language commands through several key components:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech Recognition"}),": Converting spoken language to text"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Intent Classification"}),": Understanding the user's intention from the text"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action Mapping"}),": Converting the intent to specific robot actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Execution"}),": Sending commands to the robot's action servers"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"These systems form the foundation for more advanced cognitive planning and autonomous capabilities."}),"\n",(0,o.jsx)(n.h2,{id:"whisper-api-integration-for-speech-recognition",children:"Whisper API Integration for Speech Recognition"}),"\n",(0,o.jsx)(n.p,{children:"The OpenAI Whisper API provides state-of-the-art speech recognition capabilities that can be integrated into robotics applications. Here's how to implement it:"}),"\n",(0,o.jsx)(n.h3,{id:"basic-whisper-integration",children:"Basic Whisper Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import openai\nimport asyncio\nfrom typing import Dict, Any\n\nclass WhisperVoiceRecognizer:\n    def __init__(self, api_key: str):\n        openai.api_key = api_key\n\n    async def recognize_speech(self, audio_file_path: str) -> Dict[str, Any]:\n        """\n        Convert audio file to text using Whisper API\n        """\n        try:\n            with open(audio_file_path, "rb") as audio_file:\n                transcript = await openai.Audio.atranscribe(\n                    model="whisper-1",\n                    file=audio_file,\n                    response_format="verbose_json",\n                    timestamp_granularities=["segment"]\n                )\n\n            return {\n                "text": transcript.text,\n                "confidence": transcript.avg_logprob if hasattr(transcript, \'avg_logprob\') else 0.8,  # Default confidence\n                "language": transcript.language,\n                "duration": transcript.duration\n            }\n        except Exception as e:\n            print(f"Error in speech recognition: {str(e)}")\n            return {\n                "text": "",\n                "confidence": 0.0,\n                "language": "unknown",\n                "duration": 0.0\n            }\n'})}),"\n",(0,o.jsx)(n.h3,{id:"real-time-audio-processing",children:"Real-time Audio Processing"}),"\n",(0,o.jsx)(n.p,{children:"For real-time applications, you'll need to capture audio and process it in chunks:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import pyaudio\nimport wave\nimport tempfile\nimport os\nfrom datetime import datetime\n\nclass RealTimeVoiceProcessor:\n    def __init__(self, recognizer: WhisperVoiceRecognizer):\n        self.recognizer = recognizer\n        self.chunk = 1024  # Record in chunks of 1024 samples\n        self.format = pyaudio.paInt16  # 16 bits per sample\n        self.channels = 1  # Mono\n        self.rate = 44100  # Sampling rate in Hz\n\n    def record_audio_clip(self, duration: int = 5) -> str:\n        """\n        Record audio for specified duration and save to temporary file\n        """\n        p = pyaudio.PyAudio()\n\n        # Open stream\n        stream = p.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        print(f"Recording for {duration} seconds...")\n        frames = []\n\n        for i in range(0, int(self.rate / self.chunk * duration)):\n            data = stream.read(self.chunk)\n            frames.append(data)\n\n        print("Recording finished.")\n\n        # Stop and close the stream\n        stream.stop_stream()\n        stream.close()\n        p.terminate()\n\n        # Save the recorded data as a WAV file\n        temp_filename = os.path.join(tempfile.gettempdir(), f"recording_{datetime.now().strftime(\'%Y%m%d_%H%M%S\')}.wav")\n\n        wf = wave.open(temp_filename, \'wb\')\n        wf.setnchannels(self.channels)\n        wf.setsampwidth(p.get_sample_size(self.format))\n        wf.setframerate(self.rate)\n        wf.writeframes(b\'\'.join(frames))\n        wf.close()\n\n        return temp_filename\n'})}),"\n",(0,o.jsx)(n.h2,{id:"natural-language-processing-for-command-interpretation",children:"Natural Language Processing for Command Interpretation"}),"\n",(0,o.jsx)(n.p,{children:"Once we have the text from speech recognition, we need to interpret the command and extract the intent:"}),"\n",(0,o.jsx)(n.h3,{id:"intent-classification-with-llm",children:"Intent Classification with LLM"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import openai\nfrom typing import Dict, List, Optional\n\nclass CommandInterpreter:\n    def __init__(self, api_key: str):\n        openai.api_key = api_key\n        self.valid_intents = ["NAVIGATE", "MANIPULATE", "PERCEIVE", "QUERY", "WAIT"]\n\n    async def classify_intent(self, text: str, context: Dict[str, Any] = None) -> Dict[str, Any]:\n        """\n        Use GPT to classify the intent of a command\n        """\n        if not text.strip():\n            return {\n                "intent": "UNKNOWN",\n                "parameters": {},\n                "confidence": 0.0\n            }\n\n        # Prepare context information\n        context_info = ""\n        if context:\n            if "robotPosition" in context:\n                pos = context["robotPosition"]\n                context_info += f"The robot is currently at position ({pos.get(\'x\', 0)}, {pos.get(\'y\', 0)}, {pos.get(\'z\', 0)}). "\n\n            if "environment" in context:\n                context_info += f"The environment contains: {context[\'environment\']}. "\n\n        # Create a structured prompt for intent classification\n        prompt = f"""\n        You are a robot command interpreter. Given the following user command and context,\n        classify the intent and extract relevant parameters.\n\n        Context: {context_info}\n        User Command: "{text}"\n\n        Available intents: {\', \'.join(self.valid_intents)}\n\n        Respond in the following JSON format:\n        {{\n            "intent": "...",\n            "parameters": {{}},\n            "confidence": 0.0-1.0\n        }}\n\n        For NAVIGATE intent, parameters should include direction, distance, or destination.\n        For MANIPULATE intent, parameters should include object to manipulate and action.\n        For PERCEIVE intent, parameters should include what to perceive.\n        For QUERY intent, parameters should include what is being asked.\n        For WAIT intent, parameters should include duration if specified.\n        """\n\n        try:\n            response = await openai.ChatCompletion.acreate(\n                model="gpt-3.5-turbo",\n                messages=[\n                    {"role": "system", "content": "You are a precise robot command interpreter. Respond only with valid JSON."},\n                    {"role": "user", "content": prompt}\n                ],\n                temperature=0.1,  # Low temperature for more consistent responses\n                max_tokens=200\n            )\n\n            import json\n            result = json.loads(response.choices[0].message.content)\n\n            # Validate the intent\n            if result["intent"] not in self.valid_intents:\n                result["intent"] = "UNKNOWN"\n                result["confidence"] = 0.0\n\n            return result\n\n        except Exception as e:\n            print(f"Error in intent classification: {str(e)}")\n            return {\n                "intent": "UNKNOWN",\n                "parameters": {},\n                "confidence": 0.0\n            }\n'})}),"\n",(0,o.jsx)(n.h2,{id:"mapping-voice-commands-to-ros-2-actions",children:"Mapping Voice Commands to ROS 2 Actions"}),"\n",(0,o.jsx)(n.p,{children:"The final step is to map the interpreted commands to specific ROS 2 actions:"}),"\n",(0,o.jsx)(n.h3,{id:"ros-2-action-client-for-navigation",children:"ROS 2 Action Client for Navigation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.action import ActionClient\nfrom rclpy.node import Node\nfrom nav2_msgs.action import NavigateToPose\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String\nimport math\n\nclass VoiceCommandExecutor(Node):\n    def __init__(self):\n        super().__init__('voice_command_executor')\n\n        # Create action clients for different types of actions\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n\n        # Publisher for status updates\n        self.status_pub = self.create_publisher(String, 'voice_command_status', 10)\n\n        self.get_logger().info(\"Voice Command Executor initialized\")\n\n    def execute_navigation_command(self, params: Dict[str, Any]) -> bool:\n        \"\"\"\n        Execute navigation commands based on parameters\n        \"\"\"\n        # Wait for action server\n        if not self.nav_client.wait_for_server(timeout_sec=5.0):\n            self.get_logger().error(\"Navigation action server not available\")\n            return False\n\n        # Create goal message based on parameters\n        goal_msg = NavigateToPose.Goal()\n\n        # Set the target pose based on parameters\n        if 'destination' in params:\n            # In a real implementation, you'd have a way to translate destination names to coordinates\n            # For now, we'll use some basic mapping\n            destination = params['destination'].lower()\n\n            if destination == 'kitchen':\n                goal_msg.pose.pose.position.x = 2.0\n                goal_msg.pose.pose.position.y = 1.0\n            elif destination == 'living room':\n                goal_msg.pose.pose.position.x = -1.0\n                goal_msg.pose.pose.position.y = 0.5\n            elif destination == 'bedroom':\n                goal_msg.pose.pose.position.x = 0.0\n                goal_msg.pose.pose.position.y = -2.0\n            else:\n                # If destination is described as coordinates or directions\n                if 'x' in params and 'y' in params:\n                    goal_msg.pose.pose.position.x = float(params['x'])\n                    goal_msg.pose.pose.position.y = float(params['y'])\n                else:\n                    # Default to moving forward if no specific destination\n                    goal_msg.pose.pose.position.x = 1.0\n                    goal_msg.pose.pose.position.y = 0.0\n\n        elif 'direction' in params and 'distance' in params:\n            # Move in a specific direction for a certain distance\n            direction = params['direction'].lower()\n            distance = float(params['distance'])\n\n            if direction in ['forward', 'ahead', 'north']:\n                goal_msg.pose.pose.position.x = distance\n            elif direction in ['backward', 'back', 'south']:\n                goal_msg.pose.pose.position.x = -distance\n            elif direction in ['left', 'west']:\n                goal_msg.pose.pose.position.y = distance\n            elif direction in ['right', 'east']:\n                goal_msg.pose.pose.position.y = -distance\n\n        # Set orientation to face forward\n        goal_msg.pose.pose.orientation.z = 0.0\n        goal_msg.pose.pose.orientation.w = 1.0\n\n        # Set header\n        goal_msg.pose.header.frame_id = 'map'\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\n\n        # Send goal\n        self._send_goal_future = self.nav_client.send_goal_async(\n            goal_msg,\n            feedback_callback=self.feedback_callback\n        )\n\n        self._send_goal_future.add_done_callback(self.goal_response_callback)\n\n        return True\n\n    def goal_response_callback(self, future):\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().info('Goal rejected')\n            return\n\n        self.get_logger().info('Goal accepted')\n\n        self._get_result_future = goal_handle.get_result_async()\n        self._get_result_future.add_done_callback(self.get_result_callback)\n\n    def get_result_callback(self, future):\n        result = future.result().result\n        self.get_logger().info(f'Result: {result}')\n\n        # Publish completion status\n        status_msg = String()\n        status_msg.data = \"Navigation completed\"\n        self.status_pub.publish(status_msg)\n\n    def feedback_callback(self, feedback_msg):\n        feedback = feedback_msg.feedback\n        self.get_logger().info(f'Received feedback: {feedback}')\n"})}),"\n",(0,o.jsx)(n.h2,{id:"practical-examples-of-voice-command-implementations",children:"Practical Examples of Voice Command Implementations"}),"\n",(0,o.jsx)(n.h3,{id:"complete-voice-command-system",children:"Complete Voice Command System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport tempfile\nimport os\nfrom typing import Dict, Any\n\nclass VoiceControlledRobot:\n    def __init__(self, openai_api_key: str):\n        self.whisper_recognizer = WhisperVoiceRecognizer(api_key=openai_api_key)\n        self.command_interpreter = CommandInterpreter(api_key=openai_api_key)\n        self.real_time_processor = RealTimeVoiceProcessor(self.whisper_recognizer)\n\n        # Initialize ROS 2 context\n        rclpy.init()\n        self.executor_node = VoiceCommandExecutor()\n\n        self.get_logger().info("Voice Controlled Robot System initialized")\n\n    async def process_voice_command(self, audio_path: str = None) -> bool:\n        """\n        Complete pipeline: Audio -> Text -> Intent -> Action\n        """\n        # Step 1: If no audio path provided, record live audio\n        if not audio_path:\n            audio_path = self.real_time_processor.record_audio_clip(duration=5)\n\n        # Step 2: Recognize speech\n        recognition_result = await self.whisper_recognizer.recognize_speech(audio_path)\n\n        if recognition_result["confidence"] < 0.5:  # Low confidence\n            self.get_logger().warn(f"Low confidence recognition: {recognition_result[\'confidence\']}")\n            return False\n\n        text = recognition_result["text"]\n        self.get_logger().info(f"Recognized text: {text}")\n\n        # Step 3: Interpret command\n        context = {}  # In a real system, you\'d get robot position, environment, etc.\n        interpretation_result = await self.command_interpreter.classify_intent(text, context)\n\n        if interpretation_result["confidence"] < 0.6:  # Low confidence in interpretation\n            self.get_logger().warn(f"Low confidence interpretation: {interpretation_result[\'confidence\']}")\n            return False\n\n        intent = interpretation_result["intent"]\n        params = interpretation_result["parameters"]\n\n        self.get_logger().info(f"Interpreted intent: {intent} with params: {params}")\n\n        # Step 4: Execute action based on intent\n        if intent == "NAVIGATE":\n            success = self.executor_node.execute_navigation_command(params)\n            return success\n        elif intent == "MANIPULATE":\n            # Implementation for manipulation would go here\n            self.get_logger().info("Manipulation command received but not implemented in this example")\n            return True\n        elif intent == "PERCEIVE":\n            # Implementation for perception would go here\n            self.get_logger().info("Perception command received but not implemented in this example")\n            return True\n        else:\n            self.get_logger().warn(f"Unknown or unsupported intent: {intent}")\n            return False\n\n    def get_logger(self):\n        """Helper to access logger"""\n        return self.executor_node.get_logger()\n\n    def run(self):\n        """\n        Run the voice-controlled system\n        """\n        try:\n            # In a real system, you might have a continuous listening loop\n            # For this example, we\'ll just process one command\n            loop = asyncio.get_event_loop()\n            result = loop.run_until_complete(self.process_voice_command())\n\n            if result:\n                self.get_logger().info("Command executed successfully")\n            else:\n                self.get_logger().info("Command execution failed")\n\n        except KeyboardInterrupt:\n            self.get_logger().info("Shutting down...")\n        finally:\n            # Cleanup\n            rclpy.shutdown()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"configuration-examples-for-whisper-integration",children:"Configuration Examples for Whisper Integration"}),"\n",(0,o.jsx)(n.h3,{id:"environment-configuration",children:"Environment Configuration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# .env file for API keys and configuration\nOPENAI_API_KEY=your_openai_api_key_here\nWHISPER_MODEL=whisper-1\nDEFAULT_CONFIDENCE_THRESHOLD=0.6\nRECORDING_DURATION=5  # seconds\nAUDIO_SAMPLE_RATE=44100\nAUDIO_CHANNELS=1\n"})}),"\n",(0,o.jsx)(n.h3,{id:"python-configuration",children:"Python Configuration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# config.py\nclass VoiceControlConfig:\n    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n    WHISPER_MODEL = os.getenv('WHISPER_MODEL', 'whisper-1')\n    DEFAULT_CONFIDENCE_THRESHOLD = float(os.getenv('DEFAULT_CONFIDENCE_THRESHOLD', 0.6))\n    RECORDING_DURATION = int(os.getenv('RECORDING_DURATION', 5))\n    AUDIO_SAMPLE_RATE = int(os.getenv('AUDIO_SAMPLE_RATE', 44100))\n    AUDIO_CHANNELS = int(os.getenv('AUDIO_CHANNELS', 1))\n\n    # ROS 2 related configs\n    NAVIGATION_SERVER_TIMEOUT = 5.0\n    ACTION_CLIENT_TIMEOUT = 10.0\n"})}),"\n",(0,o.jsx)(n.h2,{id:"troubleshooting-voice-recognition-issues",children:"Troubleshooting Voice Recognition Issues"}),"\n",(0,o.jsx)(n.h3,{id:"common-problems-and-solutions",children:"Common Problems and Solutions"}),"\n",(0,o.jsx)(n.h4,{id:"1-poor-recognition-quality",children:"1. Poor Recognition Quality"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Problem"}),": Whisper returns low-confidence results or incorrect text"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Causes"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Background noise in the recording"}),"\n",(0,o.jsx)(n.li,{children:"Low-quality microphone"}),"\n",(0,o.jsx)(n.li,{children:"Audio format issues"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Solutions"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Use noise cancellation techniques"}),"\n",(0,o.jsx)(n.li,{children:"Ensure microphone is positioned correctly"}),"\n",(0,o.jsx)(n.li,{children:"Verify audio format is supported (WAV, MP3, etc.)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"2-high-latency",children:"2. High Latency"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Problem"}),": Long delays between speaking and command execution"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Causes"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Network latency to Whisper API"}),"\n",(0,o.jsx)(n.li,{children:"Large audio files taking time to upload"}),"\n",(0,o.jsx)(n.li,{children:"Slow intent classification"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Solutions"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Optimize audio file size before sending"}),"\n",(0,o.jsx)(n.li,{children:"Use local Whisper models for faster processing"}),"\n",(0,o.jsx)(n.li,{children:"Implement caching for common commands"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"3-incorrect-intent-classification",children:"3. Incorrect Intent Classification"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Problem"}),": LLM misinterprets the command intent"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Causes"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Ambiguous language in the command"}),"\n",(0,o.jsx)(n.li,{children:"Insufficient context provided"}),"\n",(0,o.jsx)(n.li,{children:"Temperature settings too high"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Solutions"}),":","\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Use more specific language in prompts"}),"\n",(0,o.jsx)(n.li,{children:"Provide richer context to the LLM"}),"\n",(0,o.jsx)(n.li,{children:"Fine-tune temperature and other parameters"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"performance-optimization-tips",children:"Performance Optimization Tips"}),"\n",(0,o.jsx)(n.h4,{id:"1-audio-processing-optimization",children:"1. Audio Processing Optimization"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Preprocessing"}),": Apply noise reduction filters before sending to Whisper"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Chunking"}),": Process audio in smaller chunks for real-time applications"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Compression"}),": Use appropriate audio compression to reduce file sizes"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"2-api-call-optimization",children:"2. API Call Optimization"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Caching"}),": Cache results for common commands"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Batching"}),": Batch multiple API calls when possible"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Timeouts"}),": Implement appropriate timeouts to prevent hanging"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"exercises-for-voice-to-action-implementation",children:"Exercises for Voice-to-Action Implementation"}),"\n",(0,o.jsx)(n.h3,{id:"exercise-1-basic-voice-command-recognition",children:"Exercise 1: Basic Voice Command Recognition"}),"\n",(0,o.jsx)(n.p,{children:"Implement a basic system that can recognize and execute simple movement commands."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Set up Whisper API integration"}),"\n",(0,o.jsx)(n.li,{children:"Create a command interpreter for basic movements (forward, backward, turn)"}),"\n",(0,o.jsx)(n.li,{children:"Execute commands using ROS 2 navigation actions"}),"\n",(0,o.jsx)(n.li,{children:"Validate recognition accuracy with different speakers"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Steps"}),":"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Implement the WhisperVoiceRecognizer class"}),"\n",(0,o.jsx)(n.li,{children:"Create simple intent classification for movement commands"}),"\n",(0,o.jsx)(n.li,{children:"Connect to ROS 2 navigation system"}),"\n",(0,o.jsx)(n.li,{children:"Test with various voice commands"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"exercise-2-context-aware-command-processing",children:"Exercise 2: Context-Aware Command Processing"}),"\n",(0,o.jsx)(n.p,{children:"Extend the system to consider the robot's current state and environment."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Incorporate robot position into command interpretation"}),"\n",(0,o.jsx)(n.li,{children:"Consider environmental constraints in action planning"}),"\n",(0,o.jsx)(n.li,{children:"Handle ambiguous commands with context"}),"\n",(0,o.jsx)(n.li,{children:"Implement feedback mechanisms"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Implementation"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Extend the context parameter with robot state"}),"\n",(0,o.jsx)(n.li,{children:"Modify intent classification to consider context"}),"\n",(0,o.jsx)(n.li,{children:"Add validation checks before executing actions"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"exercise-3-error-handling-and-recovery",children:"Exercise 3: Error Handling and Recovery"}),"\n",(0,o.jsx)(n.p,{children:"Implement robust error handling for voice recognition failures."}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Detect and handle low-confidence recognitions"}),"\n",(0,o.jsx)(n.li,{children:"Implement retry mechanisms"}),"\n",(0,o.jsx)(n.li,{children:"Provide feedback to user when commands fail"}),"\n",(0,o.jsx)(n.li,{children:"Graceful degradation when system is uncertain"}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Components"}),":"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Confidence threshold checking"}),"\n",(0,o.jsx)(n.li,{children:"Retry logic with different parameters"}),"\n",(0,o.jsx)(n.li,{children:"User feedback mechanisms"}),"\n",(0,o.jsx)(n.li,{children:"Fallback command options"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"The Voice-to-Action pipeline forms the foundation of the Vision-Language-Action framework, enabling robots to understand and respond to natural language commands. By integrating Whisper API for speech recognition, LLMs for intent classification, and ROS 2 for action execution, we create a system that can bridge the gap between human language and robotic action. This foundation enables more advanced cognitive planning and autonomous capabilities explored in later chapters."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);