"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[7541],{8453(n,e,t){t.d(e,{R:()=>i,x:()=>r});var o=t(6540);const a={},s=o.createContext(a);function i(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:i(n.components),o.createElement(s.Provider,{value:e},n.children)}},9335(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4/chapter-3-autonomous-humanoid","title":"Autonomous Humanoid Capstone","description":"Learn how to integrate all VLA components into a complete autonomous humanoid system that processes commands from start to finish with navigation, perception, and manipulation.","source":"@site/docs/module-4/chapter-3-autonomous-humanoid.md","sourceDirName":"module-4","slug":"/module-4/chapter-3-autonomous-humanoid","permalink":"/docs/module-4/chapter-3-autonomous-humanoid","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module-4/chapter-3-autonomous-humanoid.md","tags":[{"inline":true,"label":"autonomous-robotics","permalink":"/docs/tags/autonomous-robotics"},{"inline":true,"label":"humanoid","permalink":"/docs/tags/humanoid"},{"inline":true,"label":"full-pipeline","permalink":"/docs/tags/full-pipeline"},{"inline":true,"label":"command-processing","permalink":"/docs/tags/command-processing"},{"inline":true,"label":"navigation","permalink":"/docs/tags/navigation"},{"inline":true,"label":"perception","permalink":"/docs/tags/perception"},{"inline":true,"label":"manipulation","permalink":"/docs/tags/manipulation"},{"inline":true,"label":"vla-integration","permalink":"/docs/tags/vla-integration"}],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Autonomous Humanoid Capstone","sidebar_position":4,"tags":["autonomous-robotics","humanoid","full-pipeline","command-processing","navigation","perception","manipulation","vla-integration"],"description":"Learn how to integrate all VLA components into a complete autonomous humanoid system that processes commands from start to finish with navigation, perception, and manipulation."},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning with LLMs","permalink":"/docs/module-4/chapter-2-cognitive-planning"}}');var a=t(4848),s=t(8453);const i={title:"Autonomous Humanoid Capstone",sidebar_position:4,tags:["autonomous-robotics","humanoid","full-pipeline","command-processing","navigation","perception","manipulation","vla-integration"],description:"Learn how to integrate all VLA components into a complete autonomous humanoid system that processes commands from start to finish with navigation, perception, and manipulation."},r="Autonomous Humanoid Capstone",l={},c=[{value:"Overview of Autonomous Humanoid Systems",id:"overview-of-autonomous-humanoid-systems",level:2},{value:"Full VLA Pipeline Integration",id:"full-vla-pipeline-integration",level:2},{value:"System Architecture",id:"system-architecture",level:3},{value:"End-to-End Command Processing Workflows",id:"end-to-end-command-processing-workflows",level:2},{value:"Complete Processing Pipeline",id:"complete-processing-pipeline",level:3},{value:"Adaptive Planning and Error Recovery",id:"adaptive-planning-and-error-recovery",level:2},{value:"Dynamic Plan Adjustment",id:"dynamic-plan-adjustment",level:3},{value:"Practical Examples of Autonomous Humanoid Implementations",id:"practical-examples-of-autonomous-humanoid-implementations",level:2},{value:"Example 1: Office Cleaning Assistant",id:"example-1-office-cleaning-assistant",level:3},{value:"Example 2: Home Assistance Robot",id:"example-2-home-assistance-robot",level:3},{value:"Example 3: Retail Customer Service Robot",id:"example-3-retail-customer-service-robot",level:3},{value:"Configuration Examples for Complete VLA Systems",id:"configuration-examples-for-complete-vla-systems",level:2},{value:"System Configuration",id:"system-configuration",level:3},{value:"Runtime Configuration",id:"runtime-configuration",level:3},{value:"Code Snippets for Integrated VLA Systems",id:"code-snippets-for-integrated-vla-systems",level:2},{value:"Main System Orchestrator",id:"main-system-orchestrator",level:3},{value:"Command Interface",id:"command-interface",level:3},{value:"Exercises for Full System Integration",id:"exercises-for-full-system-integration",level:2},{value:"Exercise 1: Complete System Integration",id:"exercise-1-complete-system-integration",level:3},{value:"Exercise 2: Multi-Step Task Execution",id:"exercise-2-multi-step-task-execution",level:3},{value:"Exercise 3: Adaptive System Behavior",id:"exercise-3-adaptive-system-behavior",level:3},{value:"Summary",id:"summary",level:2}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"autonomous-humanoid-capstone",children:"Autonomous Humanoid Capstone"})}),"\n",(0,a.jsx)(e.p,{children:"This capstone chapter brings together all the components of the Vision-Language-Action (VLA) framework into a complete autonomous humanoid system. We'll integrate voice command processing, cognitive planning with LLMs, navigation, perception, and manipulation into a cohesive system that can understand high-level goals and execute them autonomously."}),"\n",(0,a.jsx)(e.h2,{id:"overview-of-autonomous-humanoid-systems",children:"Overview of Autonomous Humanoid Systems"}),"\n",(0,a.jsx)(e.p,{children:"Autonomous humanoid systems represent the pinnacle of robotics integration, requiring seamless coordination between multiple complex subsystems:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Command Processing"}),": Understanding and interpreting high-level goals from natural language"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Cognitive Planning"}),": Decomposing complex goals into executable action sequences"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Navigation"}),": Moving the humanoid robot through the environment safely and efficiently"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Perception"}),": Sensing and understanding the environment in real-time"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Manipulation"}),": Interacting with objects in the environment to achieve goals"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Adaptive Control"}),": Adjusting behavior based on environmental changes and feedback"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"This chapter demonstrates how to integrate these components into a unified system."}),"\n",(0,a.jsx)(e.h2,{id:"full-vla-pipeline-integration",children:"Full VLA Pipeline Integration"}),"\n",(0,a.jsx)(e.h3,{id:"system-architecture",children:"System Architecture"}),"\n",(0,a.jsx)(e.p,{children:"The complete VLA pipeline follows this flow:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"Voice Command \u2192 Speech Recognition \u2192 Intent Classification \u2192 Action Planning \u2192\nNavigation \u2192 Perception \u2192 Manipulation \u2192 Execution Feedback \u2192 Goal Achievement\n"})}),"\n",(0,a.jsx)(e.p,{children:"Each component must work harmoniously to achieve autonomous behavior:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import asyncio\nimport threading\nfrom typing import Dict, Any, List, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass ExecutionState(Enum):\n    IDLE = "idle"\n    PROCESSING_COMMAND = "processing_command"\n    PLANNING_ACTIONS = "planning_actions"\n    EXECUTING_NAVIGATION = "executing_navigation"\n    EXECUTING_PERCEPTION = "executing_perception"\n    EXECUTING_MANIPULATION = "executing_manipulation"\n    ADAPTIVE_PLANNING = "adaptive_planning"\n    COMPLETED = "completed"\n    FAILED = "failed"\n\n@dataclass\nclass CommandContext:\n    """\n    Context for command execution including environment and robot state\n    """\n    robot_position: Dict[str, float]\n    environment_map: Dict[str, Any]\n    known_objects: List[Dict[str, Any]]\n    robot_capabilities: Dict[str, bool]\n    recent_interactions: List[Dict[str, Any]]\n\n@dataclass\nclass ExecutionPlan:\n    """\n    A complete plan for executing a command\n    """\n    plan_id: str\n    command: str\n    action_sequence: List[Dict[str, Any]]\n    estimated_duration: float\n    confidence: float\n\nclass AutonomousHumanoidSystem:\n    def __init__(self,\n                 whisper_recognizer: \'WhisperVoiceRecognizer\',\n                 llm_planner: \'LLMBehaviorPlanner\',\n                 action_generator: \'ROS2ActionGenerator\'):\n        self.whisper_recognizer = whisper_recognizer\n        self.llm_planner = llm_planner\n        self.action_generator = action_generator\n\n        self.current_state = ExecutionState.IDLE\n        self.active_plan = None\n        self.context = CommandContext(\n            robot_position={"x": 0.0, "y": 0.0, "z": 0.0},\n            environment_map={},\n            known_objects=[],\n            robot_capabilities={\n                "mobility": True,\n                "manipulation": True,\n                "perception": True,\n                "communication": True\n            },\n            recent_interactions=[]\n        )\n\n        # Event handlers for state changes\n        self.state_change_handlers = []\n        self.progress_callbacks = []\n\n    async def process_high_level_goal(self, command: str) -> bool:\n        """\n        Process a high-level goal and execute it autonomously\n        """\n        self._update_state(ExecutionState.PROCESSING_COMMAND)\n\n        # Step 1: If command is spoken, convert to text\n        if self._is_audio_command(command):\n            audio_path = command  # Assume command is an audio path for this example\n            recognition_result = await self.whisper_recognizer.recognize_speech(audio_path)\n            command_text = recognition_result["text"]\n            confidence = recognition_result["confidence"]\n\n            if confidence < 0.6:\n                self._log_error(f"Low confidence recognition: {confidence}")\n                return False\n        else:\n            command_text = command\n\n        self._log_info(f"Processing command: {command_text}")\n\n        # Step 2: Update context with current state\n        await self._update_context()\n\n        # Step 3: Plan actions using LLM\n        self._update_state(ExecutionState.PLANNING_ACTIONS)\n        plan = await self.llm_planner.plan_behavior(\n            command_text,\n            self.context.robot_capabilities,\n            {\n                "robot_position": self.context.robot_position,\n                "environment_map": self.context.environment_map,\n                "known_objects": self.context.known_objects\n            },\n            self.context.recent_interactions\n        )\n\n        if not plan["action_sequence"]:\n            self._log_error("No valid action sequence generated")\n            self._update_state(ExecutionState.FAILED)\n            return False\n\n        # Create execution plan\n        self.active_plan = ExecutionPlan(\n            plan_id=plan["plan_id"],\n            command=command_text,\n            action_sequence=plan["action_sequence"],\n            estimated_duration=plan.get("estimated_duration", 0.0),\n            confidence=plan.get("confidence", 0.0)\n        )\n\n        # Step 4: Execute the plan\n        success = await self._execute_plan(self.active_plan)\n\n        if success:\n            self._update_state(ExecutionState.COMPLETED)\n            self._log_info("Goal achieved successfully")\n        else:\n            self._update_state(ExecutionState.FAILED)\n            self._log_error("Goal execution failed")\n\n        return success\n\n    async def _execute_plan(self, plan: ExecutionPlan) -> bool:\n        """\n        Execute a complete action plan with monitoring and adaptation\n        """\n        self._log_info(f"Starting execution of plan: {plan.plan_id}")\n\n        for i, action in enumerate(plan.action_sequence):\n            self._log_info(f"Executing action {i+1}/{len(plan.action_sequence)}: {action[\'action_type\']}")\n\n            # Update state based on action type\n            if action[\'action_type\'] == \'NAVIGATION\':\n                self._update_state(ExecutionState.EXECUTING_NAVIGATION)\n            elif action[\'action_type\'] == \'PERCEPTION\':\n                self._update_state(ExecutionState.EXECUTING_PERCEPTION)\n            elif action[\'action_type\'] == \'MANIPULATION\':\n                self._update_state(ExecutionState.EXECUTING_MANIPULATION)\n\n            # Execute the action\n            success = await self._execute_single_adaptive_action(action)\n\n            if not success:\n                self._log_warning(f"Action {i+1} failed: {action}")\n\n                # Attempt recovery or adaptive planning\n                recovered = await self._attempt_recovery(plan, i, action)\n                if not recovered:\n                    return False\n\n            # Update context after each action\n            await self._update_context()\n\n            # Check for environmental changes that might require replanning\n            if await self._needs_adaptive_planning():\n                self._update_state(ExecutionState.ADAPTIVE_PLANNING)\n                plan = await self._adaptive_replan(plan, i)\n\n        return True\n\n    async def _execute_single_adaptive_action(self, action: Dict[str, Any]) -> bool:\n        """\n        Execute a single action with real-time adaptation\n        """\n        try:\n            # Execute the action using the action generator\n            success = await self.action_generator.execute_single_action(action)\n\n            # If successful, update context and return\n            if success:\n                await self._update_context()\n                return True\n\n            # If failed, try adaptive strategies\n            return await self._adaptive_execution_strategy(action)\n\n        except Exception as e:\n            self._log_error(f"Error executing action: {str(e)}")\n            return False\n\n    async def _attempt_recovery(self, plan: ExecutionPlan, failed_step: int, failed_action: Dict[str, Any]) -> bool:\n        """\n        Attempt to recover from a failed action\n        """\n        self._log_warning(f"Attempting recovery from failed action: {failed_action}")\n\n        # Check if this is a recurring failure\n        if self._is_recurring_failure(failed_action):\n            # Try completely different approach\n            return await self._alternative_approach(plan, failed_step, failed_action)\n\n        # Try with modified parameters\n        return await self._retry_with_modifications(plan, failed_step, failed_action)\n\n    async def _needs_adaptive_planning(self) -> bool:\n        """\n        Check if environmental changes require replanning\n        """\n        # In a real system, this would check:\n        # - Object positions have changed\n        # - New obstacles appeared\n        # - Robot state changed significantly\n        # - User context changed\n\n        # For this example, we\'ll simulate occasional environmental changes\n        import random\n        return random.random() < 0.1  # 10% chance of environmental change\n\n    async def _adaptive_replan(self, original_plan: ExecutionPlan, current_step: int) -> ExecutionPlan:\n        """\n        Replan based on changed environment\n        """\n        remaining_command = self._extract_remaining_command(original_plan, current_step)\n\n        # Get updated context\n        await self._update_context()\n\n        # Generate new plan for remaining command\n        new_plan = await self.llm_planner.plan_behavior(\n            remaining_command,\n            self.context.robot_capabilities,\n            {\n                "robot_position": self.context.robot_position,\n                "environment_map": self.context.environment_map,\n                "known_objects": self.context.known_objects\n            }\n        )\n\n        return ExecutionPlan(\n            plan_id=new_plan["plan_id"],\n            command=remaining_command,\n            action_sequence=new_plan["action_sequence"],\n            estimated_duration=new_plan.get("estimated_duration", 0.0),\n            confidence=new_plan.get("confidence", 0.0)\n        )\n\n    def _update_state(self, new_state: ExecutionState):\n        """\n        Update the system state and notify listeners\n        """\n        old_state = self.current_state\n        self.current_state = new_state\n\n        # Notify state change handlers\n        for handler in self.state_change_handlers:\n            handler(old_state, new_state)\n\n    async def _update_context(self):\n        """\n        Update the command context with current state information\n        """\n        # In a real system, this would:\n        # - Query robot position from localization system\n        # - Update environment map from SLAM\n        # - Refresh known objects from perception system\n        # - Update robot capabilities from diagnostics\n\n        # For this example, we\'ll simulate updating\n        self.context.robot_position = {\n            "x": self.context.robot_position["x"] + (random.uniform(-0.1, 0.1) if random.random() > 0.7 else 0),\n            "y": self.context.robot_position["y"] + (random.uniform(-0.1, 0.1) if random.random() > 0.7 else 0),\n            "z": 0.0\n        }\n\n        # Add to recent interactions\n        self.context.recent_interactions.append({\n            "timestamp": time.time(),\n            "event": "context_update",\n            "robot_position": self.context.robot_position.copy()\n        })\n\n        # Limit history size\n        if len(self.context.recent_interactions) > 20:\n            self.context.recent_interactions = self.context.recent_interactions[-20:]\n\n    def _is_audio_command(self, command: str) -> bool:\n        """\n        Check if command is an audio path\n        """\n        return command.endswith((\'.wav\', \'.mp3\', \'.flac\', \'.ogg\'))\n\n    def _log_info(self, message: str):\n        """\n        Log informational message\n        """\n        print(f"[INFO] {message}")\n\n    def _log_warning(self, message: str):\n        """\n        Log warning message\n        """\n        print(f"[WARNING] {message}")\n\n    def _log_error(self, message: str):\n        """\n        Log error message\n        """\n        print(f"[ERROR] {message}")\n'})}),"\n",(0,a.jsx)(e.h2,{id:"end-to-end-command-processing-workflows",children:"End-to-End Command Processing Workflows"}),"\n",(0,a.jsx)(e.h3,{id:"complete-processing-pipeline",children:"Complete Processing Pipeline"}),"\n",(0,a.jsx)(e.p,{children:"The end-to-end workflow for processing high-level commands involves multiple stages:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class EndToEndProcessor:\n    """\n    Complete end-to-end processor for autonomous humanoid commands\n    """\n    def __init__(self, autonomous_system: AutonomousHumanoidSystem):\n        self.system = autonomous_system\n        self.pipeline_monitor = PipelineMonitor()\n\n    async def process_command_pipeline(self, raw_command: str) -> Dict[str, Any]:\n        """\n        Complete pipeline: Raw command \u2192 Processed result\n        """\n        result = {\n            "success": False,\n            "stages": [],\n            "execution_time": 0.0,\n            "confidence": 0.0,\n            "error": None\n        }\n\n        start_time = time.time()\n\n        try:\n            # Stage 1: Command Reception\n            self.pipeline_monitor.start_stage("reception")\n            processed_command = await self._receive_and_validate(raw_command)\n            result["stages"].append("reception")\n            self.pipeline_monitor.end_stage("reception")\n\n            # Stage 2: Context Acquisition\n            self.pipeline_monitor.start_stage("context_acquisition")\n            await self.system._update_context()\n            result["stages"].append("context_acquisition")\n            self.pipeline_monitor.end_stage("context_acquisition")\n\n            # Stage 3: Planning\n            self.pipeline_monitor.start_stage("planning")\n            plan_success = await self.system.process_high_level_goal(processed_command)\n            result["stages"].append("planning")\n            self.pipeline_monitor.end_stage("planning")\n\n            # Stage 4: Execution\n            self.pipeline_monitor.start_stage("execution")\n            if plan_success:\n                result["success"] = True\n                result["confidence"] = self.system.active_plan.confidence if self.system.active_plan else 0.0\n            result["stages"].append("execution")\n            self.pipeline_monitor.end_stage("execution")\n\n        except Exception as e:\n            result["error"] = str(e)\n            self.system._log_error(f"Pipeline error: {str(e)}")\n        finally:\n            result["execution_time"] = time.time() - start_time\n            result["metrics"] = self.pipeline_monitor.get_metrics()\n\n        return result\n\n    async def _receive_and_validate(self, raw_command: str) -> str:\n        """\n        Receive and validate raw command\n        """\n        # Validate command format\n        if not raw_command or len(raw_command.strip()) < 3:\n            raise ValueError("Command too short")\n\n        # If it\'s an audio command, validate format\n        if self.system._is_audio_command(raw_command):\n            if not os.path.exists(raw_command):\n                raise FileNotFoundError(f"Audio file not found: {raw_command}")\n\n        return raw_command\n\nclass PipelineMonitor:\n    """\n    Monitor pipeline performance and metrics\n    """\n    def __init__(self):\n        self.stages = {}\n        self.current_stage_start = {}\n\n    def start_stage(self, stage_name: str):\n        """\n        Start timing a stage\n        """\n        self.current_stage_start[stage_name] = time.time()\n\n    def end_stage(self, stage_name: str):\n        """\n        End timing a stage\n        """\n        if stage_name in self.current_stage_start:\n            duration = time.time() - self.current_stage_start[stage_name]\n            if stage_name not in self.stages:\n                self.stages[stage_name] = []\n            self.stages[stage_name].append(duration)\n\n    def get_metrics(self) -> Dict[str, Any]:\n        """\n        Get pipeline metrics\n        """\n        metrics = {}\n        for stage_name, durations in self.stages.items():\n            if durations:\n                metrics[f"{stage_name}_avg_time"] = sum(durations) / len(durations)\n                metrics[f"{stage_name}_total_time"] = sum(durations)\n                metrics[f"{stage_name}_call_count"] = len(durations)\n\n        return metrics\n'})}),"\n",(0,a.jsx)(e.h2,{id:"adaptive-planning-and-error-recovery",children:"Adaptive Planning and Error Recovery"}),"\n",(0,a.jsx)(e.h3,{id:"dynamic-plan-adjustment",children:"Dynamic Plan Adjustment"}),"\n",(0,a.jsx)(e.p,{children:"Autonomous systems must be able to adapt to changing conditions:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class AdaptivePlanner:\n    """\n    Handles dynamic plan adjustment based on environmental changes\n    """\n    def __init__(self, base_planner: \'LLMBehaviorPlanner\'):\n        self.base_planner = base_planner\n        self.plan_history = []\n        self.failure_patterns = {}\n\n    async def adapt_plan(self,\n                        original_plan: ExecutionPlan,\n                        current_context: Dict[str, Any],\n                        reason_for_adaptation: str) -> ExecutionPlan:\n        """\n        Adapt the current plan based on new information\n        """\n        adaptation_prompt = f"""\n        The original plan needs to be adapted due to: {reason_for_adaptation}\n\n        Original command: "{original_plan.command}"\n        Original plan: {json.dumps(original_plan.action_sequence, indent=2)}\n\n        Current context:\n        - Robot position: {current_context.get(\'robot_position\', {})}\n        - Environment: {current_context.get(\'environment_map\', {})}\n        - Known objects: {current_context.get(\'known_objects\', [])}\n\n        Generate an adapted plan that accounts for the changes while still achieving the goal.\n        Respond in the same JSON format as the original plan.\n        """\n\n        try:\n            response = await openai.ChatCompletion.acreate(\n                model=self.base_planner.model,\n                messages=[\n                    {"role": "system", "content": "You are an expert in adapting robot plans to changing conditions."},\n                    {"role": "user", "content": adaptation_prompt}\n                ],\n                temperature=0.3\n            )\n\n            adapted_plan_data = json.loads(response.choices[0].message.content)\n\n            # Create new plan with adapted sequence\n            adapted_plan = ExecutionPlan(\n                plan_id=f"adapted_{original_plan.plan_id}_{int(time.time())}",\n                command=original_plan.command,\n                action_sequence=adapted_plan_data.get("action_sequence", []),\n                estimated_duration=adapted_plan_data.get("estimated_duration", original_plan.estimated_duration),\n                confidence=adapted_plan_data.get("confidence", original_plan.confidence * 0.9)  # Slightly lower confidence for adaptations\n            )\n\n            # Store in history\n            self.plan_history.append({\n                "original_plan_id": original_plan.plan_id,\n                "adapted_plan": adapted_plan,\n                "adaptation_reason": reason_for_adaptation,\n                "timestamp": time.time()\n            })\n\n            return adapted_plan\n\n        except Exception as e:\n            print(f"Error adapting plan: {str(e)}")\n            return original_plan  # Return original if adaptation fails\n\n    def learn_from_failures(self, failed_plan: ExecutionPlan, failure_reason: str):\n        """\n        Learn from plan failures to improve future planning\n        """\n        if failure_reason not in self.failure_patterns:\n            self.failure_patterns[failure_reason] = {\n                "count": 0,\n                "plans_affected": []\n            }\n\n        self.failure_patterns[failure_reason]["count"] += 1\n        self.failure_patterns[failure_reason]["plans_affected"].append(failed_plan.plan_id)\n\n    def get_adaptation_recommendations(self, current_context: Dict[str, Any]) -> List[str]:\n        """\n        Get recommendations for plan adaptation based on learned patterns\n        """\n        recommendations = []\n\n        # Check for known failure patterns that might apply\n        for pattern, data in self.failure_patterns.items():\n            if data["count"] > 3:  # If this pattern occurred more than 3 times\n                recommendations.append(f"Avoid pattern: {pattern} (occurred {data[\'count\']} times)")\n\n        return recommendations\n'})}),"\n",(0,a.jsx)(e.h2,{id:"practical-examples-of-autonomous-humanoid-implementations",children:"Practical Examples of Autonomous Humanoid Implementations"}),"\n",(0,a.jsx)(e.h3,{id:"example-1-office-cleaning-assistant",children:"Example 1: Office Cleaning Assistant"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class OfficeCleaningAssistant:\n    """\n    Example implementation: Autonomous office cleaning\n    """\n    def __init__(self, autonomous_system: AutonomousHumanoidSystem):\n        self.system = autonomous_system\n\n    async def execute_cleaning_task(self, command: str = "Clean up the office") -> bool:\n        """\n        Execute an office cleaning task\n        """\n        # Enhanced command with context\n        enhanced_command = f"""\n        {command}. Specifically:\n        - Collect all trash items and dispose of them in the waste bin\n        - Organize scattered papers on desks\n        - Return misplaced items to their proper locations\n        - Navigate safely around people and furniture\n        - Use gentle manipulation to avoid damage\n        """\n\n        # Update context with office-specific information\n        self.system.context.environment_map.update({\n            "trash_bins": [{"location": {"x": 5.0, "y": 3.0}, "type": "waste"}],\n            "desks": [{"location": {"x": 2.0, "y": 1.0}, "area": "workspace_1"}],\n            "corridors": [{"start": {"x": 0, "y": 0}, "end": {"x": 10, "y": 0}}],\n            "charging_station": {"location": {"x": -2.0, "y": -2.0}}\n        })\n\n        # Execute the cleaning task\n        success = await self.system.process_high_level_goal(enhanced_command)\n\n        if success:\n            # Return to charging station\n            return await self._return_to_charging()\n\n        return success\n\n    async def _return_to_charging(self) -> bool:\n        """\n        Return to charging station after task completion\n        """\n        charge_command = "Navigate to the charging station at coordinates x=-2.0, y=-2.0"\n        return await self.system.process_high_level_goal(charge_command)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"example-2-home-assistance-robot",children:"Example 2: Home Assistance Robot"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class HomeAssistanceRobot:\n    """\n    Example implementation: Home assistance for elderly care\n    """\n    def __init__(self, autonomous_system: AutonomousHumanoidSystem):\n        self.system = autonomous_system\n        self.resident_profile = {\n            "preferences": {"volume": 0.7, "speaking_speed": "normal"},\n            "schedule": {"medication_times": ["08:00", "12:00", "18:00"]},\n            "safe_zones": ["living_room", "kitchen", "bedroom"],\n            "forbidden_areas": ["attic", "basement_when_dark"]\n        }\n\n    async def execute_daily_assistance(self) -> bool:\n        """\n        Execute daily assistance tasks\n        """\n        # Morning routine\n        morning_tasks = [\n            "Check if the resident is awake",\n            "Prepare a glass of water and bring it to the bedroom",\n            "Announce the weather and today\'s schedule",\n            "Check medication schedule and remind about 8 AM dose"\n        ]\n\n        all_success = True\n        for task in morning_tasks:\n            success = await self.system.process_high_level_goal(task)\n            all_success = all_success and success\n            if not success:\n                self.system._log_warning(f"Task failed: {task}")\n\n        return all_success\n\n    async def emergency_response(self, emergency_type: str) -> bool:\n        """\n        Handle emergency situations\n        """\n        if emergency_type == "medical_emergency":\n            command = "Immediately go to the resident\'s location and call for medical assistance"\n        elif emergency_type == "fall_detection":\n            command = "Go to the detected fall location, assess situation, and call for help"\n        elif emergency_type == "fire_alarm":\n            command = "Evacuate the resident to the predetermined safe location"\n        else:\n            command = f"Handle {emergency_type} situation according to protocol"\n\n        # Execute emergency command with highest priority\n        self.system.context.robot_capabilities["priority"] = "emergency"\n        return await self.system.process_high_level_goal(command)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"example-3-retail-customer-service-robot",children:"Example 3: Retail Customer Service Robot"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class RetailCustomerServiceRobot:\n    """\n    Example implementation: Customer service in retail environment\n    """\n    def __init__(self, autonomous_system: AutonomousHumanoidSystem):\n        self.system = autonomous_system\n        self.store_layout = {\n            "departments": {\n                "electronics": {"section_id": "ELEC001", "coordinates": {"x": 10, "y": 5}},\n                "clothing": {"section_id": "CLOT001", "coordinates": {"x": 15, "y": 8}},\n                "checkout": {"section_id": "CHK001", "coordinates": {"x": 20, "y": 2}}\n            },\n            "product_locations": {\n                "smartphone": {"department": "electronics", "aisle": "A1", "shelf": "top"},\n                "dress": {"department": "clothing", "aisle": "B2", "rack": "left"}\n            },\n            "customer_flow": ["entrance", "electronics", "clothing", "checkout"]\n        }\n\n    async def assist_customer(self, customer_request: str) -> bool:\n        """\n        Assist customers with various requests\n        """\n        # Enrich the request with store context\n        contextualized_request = f"""\n        {customer_request}. Use your knowledge of the store layout:\n        - Departments: {list(self.store_layout[\'departments\'].keys())}\n        - Product locations: {list(self.store_layout[\'product_locations\'].keys())[:5]} (showing first 5)\n        - Typical customer flow: {self.store_layout[\'customer_flow\']}\n\n        Provide helpful guidance and navigation assistance.\n        """\n\n        # Update context with store information\n        self.system.context.environment_map.update(self.store_layout)\n\n        return await self.system.process_high_level_goal(contextualized_request)\n\n    async def restock_assistant(self) -> bool:\n        """\n        Assist with restocking shelves\n        """\n        # Get inventory alerts\n        inventory_alerts = self._get_inventory_alerts()\n\n        for alert in inventory_alerts:\n            if alert["low_stock"]:\n                product = alert["product"]\n                location = self.store_layout["product_locations"].get(product, {})\n\n                if location:\n                    command = f"Navigate to {location[\'department\']} department, aisle {location.get(\'aisle\', \'unknown\')}, " \\\n                             f"and check the stock level of {product}. Report findings."\n\n                    success = await self.system.process_high_level_goal(command)\n                    if not success:\n                        self.system._log_error(f"Restock check failed for {product}")\n\n        return True\n\n    def _get_inventory_alerts(self) -> List[Dict[str, Any]]:\n        """\n        Get inventory alerts from store system (simulated)\n        """\n        # Simulated inventory alerts\n        import random\n        alerts = []\n\n        if random.random() > 0.7:  # 30% chance of an alert\n            alerts.append({\n                "product": "smartphone",\n                "low_stock": True,\n                "current_quantity": 2,\n                "reorder_threshold": 5\n            })\n\n        return alerts\n'})}),"\n",(0,a.jsx)(e.h2,{id:"configuration-examples-for-complete-vla-systems",children:"Configuration Examples for Complete VLA Systems"}),"\n",(0,a.jsx)(e.h3,{id:"system-configuration",children:"System Configuration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# autonomous_config.py\nclass AutonomousConfig:\n    # Core system parameters\n    SYSTEM_NAME = os.getenv('SYSTEM_NAME', 'AutonomousHumanoid')\n    ROBOT_TYPE = os.getenv('ROBOT_TYPE', 'humanoid')\n\n    # Processing parameters\n    CONFIDENCE_THRESHOLD = float(os.getenv('CONFIDENCE_THRESHOLD', 0.7))\n    MAX_EXECUTION_TIME = int(os.getenv('MAX_EXECUTION_TIME', 300))  # 5 minutes\n    MAX_PLAN_STEPS = int(os.getenv('MAX_PLAN_STEPS', 50))\n\n    # Adaptation parameters\n    ENVIRONMENT_CHANGE_THRESHOLD = float(os.getenv('ENVIRONMENT_CHANGE_THRESHOLD', 0.1))\n    ADAPTATION_INTERVAL = int(os.getenv('ADAPTATION_INTERVAL', 30))  # seconds\n\n    # Recovery parameters\n    MAX_RECOVERY_ATTEMPTS = int(os.getenv('MAX_RECOVERY_ATTEMPTS', 3))\n    RECOVERY_DELAY = float(os.getenv('RECOVERY_DELAY', 2.0))\n\n    # Safety parameters\n    SAFETY_TIMEOUT = int(os.getenv('SAFETY_TIMEOUT', 60))\n    EMERGENCY_STOP_ENABLED = os.getenv('EMERGENCY_STOP_ENABLED', 'true').lower() == 'true'\n    SAFE_ZONES_ONLY = os.getenv('SAFE_ZONES_ONLY', 'false').lower() == 'true'\n\n    # Component-specific configs\n    class Whisper:\n        MODEL = os.getenv('WHISPER_MODEL', 'whisper-1')\n        CONFIDENCE_THRESHOLD = float(os.getenv('WHISPER_CONFIDENCE', 0.6))\n        LANGUAGE = os.getenv('WHISPER_LANGUAGE', 'en')\n\n    class LLM:\n        MODEL = os.getenv('LLM_MODEL', 'gpt-4')\n        TEMPERATURE = float(os.getenv('LLM_TEMPERATURE', 0.2))\n        MAX_TOKENS = int(os.getenv('LLM_MAX_TOKENS', 1000))\n        PLANNING_CONFIDENCE = float(os.getenv('LLM_PLANNING_CONFIDENCE', 0.7))\n\n    class Navigation:\n        MAX_SPEED = float(os.getenv('NAV_MAX_SPEED', 0.5))\n        MIN_DISTANCE_TO_OBSTACLE = float(os.getenv('NAV_MIN_OBSTACLE_DIST', 0.3))\n        RECOVERY_BEHAVIORS = os.getenv('NAV_RECOVERY', 'true').lower() == 'true'\n"})}),"\n",(0,a.jsx)(e.h3,{id:"runtime-configuration",children:"Runtime Configuration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# runtime_config.py\nclass RuntimeProfile:\n    """\n    Different operational profiles for different scenarios\n    """\n    PROFILES = {\n        "conservative": {\n            "confidence_multiplier": 1.2,  # Require higher confidence\n            "execution_speed": 0.7,  # Slower execution\n            "planning_horizon": 5,  # Shorter term planning\n            "safety_margin": 1.5,  # Wider safety margins\n            "verification_steps": True  # More verification\n        },\n        "aggressive": {\n            "confidence_multiplier": 0.8,  # Lower confidence requirement\n            "execution_speed": 1.5,  # Faster execution\n            "planning_horizon": 15,  # Longer term planning\n            "safety_margin": 0.8,  # Tighter margins\n            "verification_steps": False  # Fewer verifications\n        },\n        "standard": {\n            "confidence_multiplier": 1.0,\n            "execution_speed": 1.0,\n            "planning_horizon": 10,\n            "safety_margin": 1.0,\n            "verification_steps": True\n        },\n        "emergency": {\n            "confidence_multiplier": 0.5,  # Lower threshold for urgent responses\n            "execution_speed": 2.0,  # Maximum speed\n            "planning_horizon": 3,  # Immediate actions only\n            "safety_margin": 0.5,  # Minimal safety for emergencies\n            "verification_steps": False,\n            "priority": "highest"\n        }\n    }\n\ndef get_runtime_config(profile_name: str = "standard") -> Dict[str, Any]:\n    """\n    Get configuration based on operational profile\n    """\n    base_config = vars(AutonomousConfig)\n    profile_config = RuntimeProfile.PROFILES.get(profile_name, RuntimeProfile.PROFILES["standard"])\n\n    # Merge configurations\n    merged_config = {**base_config, **profile_config}\n    merged_config["profile"] = profile_name\n\n    return merged_config\n'})}),"\n",(0,a.jsx)(e.h2,{id:"code-snippets-for-integrated-vla-systems",children:"Code Snippets for Integrated VLA Systems"}),"\n",(0,a.jsx)(e.h3,{id:"main-system-orchestrator",children:"Main System Orchestrator"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import signal\nimport sys\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass VLAOrchestrator:\n    """\n    Main orchestrator for the complete VLA system\n    """\n    def __init__(self, config: AutonomousConfig):\n        self.config = config\n        self.is_running = False\n        self.command_queue = asyncio.Queue()\n        self.executor = ThreadPoolExecutor(max_workers=4)\n\n        # Initialize components\n        self.whisper_recognizer = WhisperVoiceRecognizer(api_key=config.LLM_MODEL)\n        self.llm_planner = LLMBehaviorPlanner(api_key=config.LLM_MODEL, model=config.LLM.MODEL)\n        self.action_generator = ROS2ActionGenerator()  # Assuming ROS2 node\n        self.autonomous_system = AutonomousHumanoidSystem(\n            self.whisper_recognizer,\n            self.llm_planner,\n            self.action_generator\n        )\n\n        # Register shutdown handler\n        signal.signal(signal.SIGINT, self._signal_handler)\n        signal.signal(signal.SIGTERM, self._signal_handler)\n\n    async def start(self):\n        """\n        Start the autonomous system\n        """\n        self.is_running = True\n        self.autonomous_system._log_info("Starting VLA Orchestrator...")\n\n        # Start the main processing loop\n        await self._main_processing_loop()\n\n    async def _main_processing_loop(self):\n        """\n        Main processing loop that handles commands and system state\n        """\n        self.autonomous_system._log_info("VLA Orchestrator running. Waiting for commands...")\n\n        while self.is_running:\n            try:\n                # Check for new commands\n                if not self.command_queue.empty():\n                    command = await self.command_queue.get()\n                    await self._process_command(command)\n\n                # Perform periodic system checks\n                await self._perform_system_health_check()\n\n                # Small delay to prevent busy waiting\n                await asyncio.sleep(0.1)\n\n            except Exception as e:\n                self.autonomous_system._log_error(f"Error in main loop: {str(e)}")\n                await asyncio.sleep(1)  # Brief pause before continuing\n\n    async def _process_command(self, command: str):\n        """\n        Process a single command through the VLA pipeline\n        """\n        try:\n            self.autonomous_system._log_info(f"Processing command: {command[:50]}...")\n\n            # Process through end-to-end pipeline\n            processor = EndToEndProcessor(self.autonomous_system)\n            result = await processor.process_command_pipeline(command)\n\n            if result["success"]:\n                self.autonomous_system._log_info(f"Command completed successfully: {command[:30]}...")\n            else:\n                self.autonomous_system._log_error(f"Command failed: {result[\'error\']}")\n\n        except Exception as e:\n            self.autonomous_system._log_error(f"Command processing error: {str(e)}")\n\n    async def _perform_system_health_check(self):\n        """\n        Perform periodic health checks\n        """\n        # Check if all components are responsive\n        # In a real system, this would check:\n        # - LLM API connectivity\n        # - ROS2 node status\n        # - Sensor availability\n        # - Battery level\n        # - Navigation system status\n\n        # For this example, just log periodically\n        if int(time.time()) % 30 == 0:  # Every 30 seconds\n            self.autonomous_system._log_info(f"System health check - State: {self.autonomous_system.current_state.value}")\n\n    def add_command(self, command: str):\n        """\n        Add a command to the processing queue\n        """\n        asyncio.create_task(self.command_queue.put(command))\n\n    def _signal_handler(self, signum, frame):\n        """\n        Handle shutdown signals\n        """\n        self.autonomous_system._log_info(f"Received signal {signum}, shutting down...")\n        self.is_running = False\n        self.executor.shutdown(wait=True)\n        sys.exit(0)\n\n    async def shutdown(self):\n        """\n        Gracefully shut down the system\n        """\n        self.is_running = False\n        self.executor.shutdown(wait=True)\n        self.autonomous_system._log_info("VLA Orchestrator shut down gracefully.")\n'})}),"\n",(0,a.jsx)(e.h3,{id:"command-interface",children:"Command Interface"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class CommandInterface:\n    """\n    Interface for accepting commands from various sources\n    """\n    def __init__(self, orchestrator: VLAOrchestrator):\n        self.orchestrator = orchestrator\n\n    def accept_voice_command(self, audio_path: str):\n        """\n        Accept voice command from audio file\n        """\n        self.orchestrator.add_command(audio_path)\n\n    def accept_text_command(self, text: str):\n        """\n        Accept text command directly\n        """\n        self.orchestrator.add_command(text)\n\n    def accept_multimodal_command(self, text: str, context_data: Dict[str, Any] = None):\n        """\n        Accept command with additional context data\n        """\n        # Combine text with context information\n        if context_data:\n            command_with_context = {\n                "command": text,\n                "context": context_data\n            }\n            # For now, just pass the text - context would be handled by the system\n            self.orchestrator.add_command(text)\n        else:\n            self.orchestrator.add_command(text)\n\n# Example usage\nasync def main():\n    """\n    Example main function showing how to use the system\n    """\n    config = AutonomousConfig()\n    orchestrator = VLAOrchestrator(config)\n    interface = CommandInterface(orchestrator)\n\n    try:\n        # Add some example commands\n        interface.accept_text_command("Clean up the workspace by organizing papers and disposing of trash")\n        interface.accept_text_command("Go to the kitchen and bring me a glass of water")\n        interface.accept_text_command("Find my keys and bring them to me")\n\n        # Start the orchestrator\n        await orchestrator.start()\n\n    except KeyboardInterrupt:\n        print("\\nShutting down...")\n        await orchestrator.shutdown()\n\nif __name__ == "__main__":\n    asyncio.run(main())\n'})}),"\n",(0,a.jsx)(e.h2,{id:"exercises-for-full-system-integration",children:"Exercises for Full System Integration"}),"\n",(0,a.jsx)(e.h3,{id:"exercise-1-complete-system-integration",children:"Exercise 1: Complete System Integration"}),"\n",(0,a.jsx)(e.p,{children:"Integrate all VLA components into a working autonomous humanoid system."}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Integrate voice recognition, LLM planning, and action execution"}),"\n",(0,a.jsx)(e.li,{children:"Create a complete pipeline from voice command to robot action"}),"\n",(0,a.jsx)(e.li,{children:"Implement basic error handling and recovery"}),"\n",(0,a.jsx)(e.li,{children:'Test with simple commands like "move forward" or "turn left"'}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Steps"}),":"]}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Implement the AutonomousHumanoidSystem class"}),"\n",(0,a.jsx)(e.li,{children:"Connect all components: voice \u2192 LLM \u2192 actions"}),"\n",(0,a.jsx)(e.li,{children:"Create a simple test environment"}),"\n",(0,a.jsx)(e.li,{children:"Test with basic commands"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"exercise-2-multi-step-task-execution",children:"Exercise 2: Multi-Step Task Execution"}),"\n",(0,a.jsx)(e.p,{children:"Extend the system to handle complex multi-step tasks."}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Execute commands that require multiple navigation, perception, and manipulation steps"}),"\n",(0,a.jsx)(e.li,{children:"Maintain state between actions"}),"\n",(0,a.jsx)(e.li,{children:"Handle intermediate failures and recovery"}),"\n",(0,a.jsx)(e.li,{children:'Test with commands like "Go to the kitchen, find a red cup, and bring it to the living room"'}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Implementation"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Implement the EndToEndProcessor"}),"\n",(0,a.jsx)(e.li,{children:"Create state management between actions"}),"\n",(0,a.jsx)(e.li,{children:"Add error recovery mechanisms"}),"\n",(0,a.jsx)(e.li,{children:"Test with multi-step commands"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"exercise-3-adaptive-system-behavior",children:"Exercise 3: Adaptive System Behavior"}),"\n",(0,a.jsx)(e.p,{children:"Implement adaptation to environmental changes."}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Detect when environmental conditions change during task execution"}),"\n",(0,a.jsx)(e.li,{children:"Adjust plans dynamically based on new information"}),"\n",(0,a.jsx)(e.li,{children:"Learn from failures to improve future performance"}),"\n",(0,a.jsx)(e.li,{children:"Test with simulated environmental changes"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Components"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Environmental change detection"}),"\n",(0,a.jsx)(e.li,{children:"Dynamic plan adaptation"}),"\n",(0,a.jsx)(e.li,{children:"Failure learning and pattern recognition"}),"\n",(0,a.jsx)(e.li,{children:"Continuous improvement mechanisms"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"The Autonomous Humanoid Capstone chapter demonstrates the complete integration of the Vision-Language-Action framework into a unified autonomous system. By combining voice command processing, cognitive planning with LLMs, navigation, perception, and manipulation, we create a sophisticated system capable of understanding high-level goals and executing them autonomously."}),"\n",(0,a.jsx)(e.p,{children:"The key components of the autonomous system include:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Complete end-to-end command processing pipeline"}),"\n",(0,a.jsx)(e.li,{children:"Adaptive planning and error recovery mechanisms"}),"\n",(0,a.jsx)(e.li,{children:"Integration of all VLA components into a cohesive system"}),"\n",(0,a.jsx)(e.li,{children:"Monitoring and health checking for reliable operation"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"This capstone implementation represents the culmination of the VLA module, showing how individual components work together to create intelligent, responsive robotic systems that can understand and execute natural language commands in complex environments. The system demonstrates the potential of combining modern AI techniques with robotics to create truly autonomous agents."})]})}function d(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(m,{...n})}):m(n)}}}]);